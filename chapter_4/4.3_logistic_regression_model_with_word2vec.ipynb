{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "382dc225",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91c5f9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_IN_PATH = './data_in/'\n",
    "TRAIN_CLEAN_DATA = 'train_clean.csv'\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "TEST_SPLIT = 0.2\n",
    "\n",
    "train_data = pd.read_csv(DATA_IN_PATH + TRAIN_CLEAN_DATA)\n",
    "\n",
    "reviews = list(train_data['review'])\n",
    "sentiments = list(train_data['sentiment'])\n",
    "\n",
    "sentences = []\n",
    "\n",
    "for review in reviews:\n",
    "    sentences.append(review.split())\n",
    "    \n",
    "num_features = 300  # 워드 벡터 특징값 수\n",
    "min_word_count = 40 # 단어에 대한 최소 빈도 수\n",
    "num_workers = 4     # 프로세스 개수\n",
    "context = 10        # 컨텍스트 윈도 크기\n",
    "downsampling = 1e-3 # 다운샘플링 비율"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d358d0",
   "metadata": {},
   "source": [
    "## word2vec 모델의 하이퍼파라미터\n",
    "- num_features : 각 단어의 임베딩 벡터 차원\n",
    "- min_word_count : 모델에 의미 있는 단어를 가지고 학습하기 위해 적은 빈도 수의 단어들은 학습하지 않는다.\n",
    "- num_workers : 모델 학습 시 학습을 위한 프로세스 개수를 지정\n",
    "- context : word2vec 수행하기 위한 컨텍스트 윈도 크기 지정\n",
    "- downsampling : word2vec 학습을 수행할 때 빠른 학습을 위해 정답 단어 라벨에 대한 다운샘플링 비율을 지정한다. 보통 0.001이 좋은 성능을 낸다고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3827dc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\n",
    "                    # 로그 보여줄 형식\n",
    "                   level=logging.INFO) # 로그 수준"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a579336",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-08 13:41:05,730 : INFO : collecting all words and their counts\n",
      "2023-04-08 13:41:05,739 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2023-04-08 13:41:05,961 : INFO : PROGRESS: at sentence #10000, processed 1205223 words, keeping 51374 word types\n",
      "2023-04-08 13:41:06,186 : INFO : PROGRESS: at sentence #20000, processed 2396605 words, keeping 67660 word types\n",
      "2023-04-08 13:41:06,305 : INFO : collected 74065 word types from a corpus of 2988089 raw words and 25000 sentences\n",
      "2023-04-08 13:41:06,306 : INFO : Creating a fresh vocabulary\n",
      "2023-04-08 13:41:06,357 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=40 retains 8160 unique words (11.017349625329103%% of original 74065, drops 65905)', 'datetime': '2023-04-08T13:41:06.357318', 'gensim': '4.1.2', 'python': '3.7.13 (default, Mar 28 2022, 08:03:21) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'prepare_vocab'}\n",
      "2023-04-08 13:41:06,358 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=40 leaves 2627273 word corpus (87.92485765986221%% of original 2988089, drops 360816)', 'datetime': '2023-04-08T13:41:06.358323', 'gensim': '4.1.2', 'python': '3.7.13 (default, Mar 28 2022, 08:03:21) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'prepare_vocab'}\n",
      "2023-04-08 13:41:06,400 : INFO : deleting the raw counts dictionary of 74065 items\n",
      "2023-04-08 13:41:06,402 : INFO : sample=0.001 downsamples 30 most-common words\n",
      "2023-04-08 13:41:06,403 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 2494384.49928802 word corpus (94.9%% of prior 2627273)', 'datetime': '2023-04-08T13:41:06.403648', 'gensim': '4.1.2', 'python': '3.7.13 (default, Mar 28 2022, 08:03:21) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'prepare_vocab'}\n",
      "2023-04-08 13:41:06,477 : INFO : estimated required memory for 8160 words and 300 dimensions: 23664000 bytes\n",
      "2023-04-08 13:41:06,477 : INFO : resetting layer weights\n",
      "2023-04-08 13:41:06,487 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-04-08T13:41:06.487963', 'gensim': '4.1.2', 'python': '3.7.13 (default, Mar 28 2022, 08:03:21) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'build_vocab'}\n",
      "2023-04-08 13:41:06,488 : INFO : Word2Vec lifecycle event {'msg': 'training model with 4 workers on 8160 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10 shrink_windows=True', 'datetime': '2023-04-08T13:41:06.488965', 'gensim': '4.1.2', 'python': '3.7.13 (default, Mar 28 2022, 08:03:21) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'train'}\n",
      "2023-04-08 13:41:07,494 : INFO : EPOCH 1 - PROGRESS: at 45.16% examples, 1131104 words/s, in_qsize 7, out_qsize 0\n",
      "2023-04-08 13:41:08,500 : INFO : EPOCH 1 - PROGRESS: at 93.47% examples, 1162746 words/s, in_qsize 7, out_qsize 0\n",
      "2023-04-08 13:41:08,613 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2023-04-08 13:41:08,620 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-04-08 13:41:08,625 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-04-08 13:41:08,627 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-04-08 13:41:08,628 : INFO : EPOCH - 1 : training on 2988089 raw words (2494533 effective words) took 2.1s, 1168364 effective words/s\n",
      "2023-04-08 13:41:09,634 : INFO : EPOCH 2 - PROGRESS: at 46.80% examples, 1172133 words/s, in_qsize 7, out_qsize 0\n",
      "2023-04-08 13:41:10,634 : INFO : EPOCH 2 - PROGRESS: at 92.75% examples, 1157161 words/s, in_qsize 7, out_qsize 0\n",
      "2023-04-08 13:41:10,775 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2023-04-08 13:41:10,778 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-04-08 13:41:10,780 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-04-08 13:41:10,786 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-04-08 13:41:10,787 : INFO : EPOCH - 2 : training on 2988089 raw words (2494428 effective words) took 2.2s, 1157511 effective words/s\n",
      "2023-04-08 13:41:11,799 : INFO : EPOCH 3 - PROGRESS: at 44.45% examples, 1106813 words/s, in_qsize 8, out_qsize 0\n",
      "2023-04-08 13:41:12,809 : INFO : EPOCH 3 - PROGRESS: at 88.21% examples, 1095483 words/s, in_qsize 7, out_qsize 0\n",
      "2023-04-08 13:41:13,043 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2023-04-08 13:41:13,046 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-04-08 13:41:13,055 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-04-08 13:41:13,060 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-04-08 13:41:13,060 : INFO : EPOCH - 3 : training on 2988089 raw words (2494448 effective words) took 2.3s, 1098706 effective words/s\n",
      "2023-04-08 13:41:14,072 : INFO : EPOCH 4 - PROGRESS: at 42.76% examples, 1068256 words/s, in_qsize 7, out_qsize 0\n",
      "2023-04-08 13:41:15,077 : INFO : EPOCH 4 - PROGRESS: at 86.28% examples, 1073937 words/s, in_qsize 8, out_qsize 0\n",
      "2023-04-08 13:41:15,373 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2023-04-08 13:41:15,374 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-04-08 13:41:15,380 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-04-08 13:41:15,382 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-04-08 13:41:15,382 : INFO : EPOCH - 4 : training on 2988089 raw words (2493933 effective words) took 2.3s, 1076081 effective words/s\n",
      "2023-04-08 13:41:16,393 : INFO : EPOCH 5 - PROGRESS: at 41.11% examples, 1026817 words/s, in_qsize 7, out_qsize 0\n",
      "2023-04-08 13:41:17,396 : INFO : EPOCH 5 - PROGRESS: at 82.05% examples, 1021674 words/s, in_qsize 7, out_qsize 0\n",
      "2023-04-08 13:41:17,791 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2023-04-08 13:41:17,795 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-04-08 13:41:17,800 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-04-08 13:41:17,812 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-04-08 13:41:17,813 : INFO : EPOCH - 5 : training on 2988089 raw words (2494294 effective words) took 2.4s, 1027820 effective words/s\n",
      "2023-04-08 13:41:17,813 : INFO : Word2Vec lifecycle event {'msg': 'training on 14940445 raw words (12471636 effective words) took 11.3s, 1101316 effective words/s', 'datetime': '2023-04-08T13:41:17.813616', 'gensim': '4.1.2', 'python': '3.7.13 (default, Mar 28 2022, 08:03:21) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'train'}\n",
      "2023-04-08 13:41:17,814 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec(vocab=8160, vector_size=300, alpha=0.025)', 'datetime': '2023-04-08T13:41:17.814614', 'gensim': '4.1.2', 'python': '3.7.13 (default, Mar 28 2022, 08:03:21) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "model = word2vec.Word2Vec(sentences, \n",
    "                          workers=num_workers,\n",
    "                          vector_size=num_features,\n",
    "                          min_count = min_word_count,\n",
    "                          window=context,\n",
    "                          sample=downsampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45c44d8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x14c636a45c8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e38314fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(words, model, num_features):\n",
    "    feature_vector = np.zeros((num_features), dtype=np.float32)\n",
    "    \n",
    "    num_words = 0\n",
    "    index2word_set = set(model.wv.index_to_key) # 8160길이\n",
    "    \n",
    "    for w in words:\n",
    "        if w in index2word_set:\n",
    "            num_words += 1\n",
    "            feature_vector = np.add(feature_vector, model.wv[w])\n",
    "            \n",
    "    feature_vector = np.divide(feature_vector, num_words)\n",
    "    \n",
    "    return feature_vector\n",
    "\n",
    "def get_dataset(reviews, model, num_features):\n",
    "    dataset = list()\n",
    "    \n",
    "    for s in reviews:\n",
    "        dataset.append(get_features(s, model, num_features))\n",
    "        \n",
    "    reviewFeatureVecs = np.stack(dataset)\n",
    "    \n",
    "    return reviewFeatureVecs\n",
    "\n",
    "test_data_vecs = get_dataset(sentences, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5cdc983d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stuff', 'going', 'moment', 'mj', 'started', 'listening', 'music', 'watching', 'odd', 'documentary', 'watched', 'wiz', 'watched', 'moonwalker', 'maybe', 'want', 'get', 'certain', 'insight', 'guy', 'thought', 'really', 'cool', 'eighties', 'maybe', 'make', 'mind', 'whether', 'guilty', 'innocent', 'moonwalker', 'part', 'biography', 'part', 'feature', 'film', 'remember', 'going', 'see', 'cinema', 'originally', 'released', 'subtle', 'messages', 'mj', 'feeling', 'towards', 'press', 'also', 'obvious', 'message', 'drugs', 'bad', 'kay', 'visually', 'impressive', 'course', 'michael', 'jackson', 'unless', 'remotely', 'like', 'mj', 'anyway', 'going', 'hate', 'find', 'boring', 'may', 'call', 'mj', 'egotist', 'consenting', 'making', 'movie', 'mj', 'fans', 'would', 'say', 'made', 'fans', 'true', 'really', 'nice', 'actual', 'feature', 'film', 'bit', 'finally', 'starts', 'minutes', 'excluding', 'smooth', 'criminal', 'sequence', 'joe', 'pesci', 'convincing', 'psychopathic', 'powerful', 'drug', 'lord', 'wants', 'mj', 'dead', 'bad', 'beyond', 'mj', 'overheard', 'plans', 'nah', 'joe', 'pesci', 'character', 'ranted', 'wanted', 'people', 'know', 'supplying', 'drugs', 'etc', 'dunno', 'maybe', 'hates', 'mj', 'music', 'lots', 'cool', 'things', 'like', 'mj', 'turning', 'car', 'robot', 'whole', 'speed', 'demon', 'sequence', 'also', 'director', 'must', 'patience', 'saint', 'came', 'filming', 'kiddy', 'bad', 'sequence', 'usually', 'directors', 'hate', 'working', 'one', 'kid', 'let', 'alone', 'whole', 'bunch', 'performing', 'complex', 'dance', 'scene', 'bottom', 'line', 'movie', 'people', 'like', 'mj', 'one', 'level', 'another', 'think', 'people', 'stay', 'away', 'try', 'give', 'wholesome', 'message', 'ironically', 'mj', 'bestest', 'buddy', 'movie', 'girl', 'michael', 'jackson', 'truly', 'one', 'talented', 'people', 'ever', 'grace', 'planet', 'guilty', 'well', 'attention', 'gave', 'subject', 'hmmm', 'well', 'know', 'people', 'different', 'behind', 'closed', 'doors', 'know', 'fact', 'either', 'extremely', 'nice', 'stupid', 'guy', 'one', 'sickest', 'liars', 'hope', 'latter']\n"
     ]
    }
   ],
   "source": [
    "for s in sentences:\n",
    "    print(s)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "38be0a42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8160"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(model.wv.index_to_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d9fd785",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'namely',\n",
       " 'fortunately',\n",
       " 'sun',\n",
       " 'muni',\n",
       " 'transport',\n",
       " 'blockbusters',\n",
       " 'mood',\n",
       " 'type',\n",
       " 'equipment',\n",
       " 'th',\n",
       " 'solo',\n",
       " 'transition',\n",
       " 'funky',\n",
       " 'fx',\n",
       " 'pfeiffer',\n",
       " 'feast',\n",
       " 'elderly',\n",
       " 'judy',\n",
       " 'angles',\n",
       " 'ended',\n",
       " 'amazingly',\n",
       " 'verhoeven',\n",
       " 'convoluted',\n",
       " 'capacity',\n",
       " 'cooking',\n",
       " 'taylor',\n",
       " 'benefit',\n",
       " 'steele',\n",
       " 'turner',\n",
       " 'north',\n",
       " 'monty',\n",
       " 'chain',\n",
       " 'guide',\n",
       " 'expecting',\n",
       " 'rage',\n",
       " 'prisoners',\n",
       " 'match',\n",
       " 'paz',\n",
       " 'anil',\n",
       " 'lighting',\n",
       " 'friendly',\n",
       " 'portrays',\n",
       " 'discussion',\n",
       " 'parking',\n",
       " 'wisdom',\n",
       " 'ludicrous',\n",
       " 'tribute',\n",
       " 'lend',\n",
       " 'prostitutes',\n",
       " 'tap',\n",
       " 'boobs',\n",
       " 'nutshell',\n",
       " 'rejects',\n",
       " 'hysterical',\n",
       " 'fades',\n",
       " 'lester',\n",
       " 'signature',\n",
       " 'reviewers',\n",
       " 'producer',\n",
       " 'hippie',\n",
       " 'premise',\n",
       " 'crashes',\n",
       " 'uwe',\n",
       " 'wow',\n",
       " 'maker',\n",
       " 'reminded',\n",
       " 'cole',\n",
       " 'estranged',\n",
       " 'scares',\n",
       " 'ten',\n",
       " 'conclusions',\n",
       " 'attack',\n",
       " 'entrance',\n",
       " 'guessed',\n",
       " 'errol',\n",
       " 'roughly',\n",
       " 'stumble',\n",
       " 'eager',\n",
       " 'dealing',\n",
       " 'devices',\n",
       " 'curiously',\n",
       " 'anthony',\n",
       " 'voice',\n",
       " 'haunting',\n",
       " 'advance',\n",
       " 'interests',\n",
       " 'spine',\n",
       " 'novelty',\n",
       " 'legitimate',\n",
       " 'hmm',\n",
       " 'tickets',\n",
       " 'artwork',\n",
       " 'controlled',\n",
       " 'despair',\n",
       " 'vehicle',\n",
       " 'realizing',\n",
       " 'definitely',\n",
       " 'link',\n",
       " 'seconds',\n",
       " 'christy',\n",
       " 'least',\n",
       " 'projected',\n",
       " 'brutal',\n",
       " 'model',\n",
       " 'warmth',\n",
       " 'understatement',\n",
       " 'lips',\n",
       " 'eerie',\n",
       " 'jumping',\n",
       " 'began',\n",
       " 'script',\n",
       " 'begging',\n",
       " 'eventual',\n",
       " 'truths',\n",
       " 'credibility',\n",
       " 'beowulf',\n",
       " 'moronic',\n",
       " 'classmates',\n",
       " 'lines',\n",
       " 'released',\n",
       " 'twice',\n",
       " 'seems',\n",
       " 'valuable',\n",
       " 'world',\n",
       " 'inherent',\n",
       " 'awkward',\n",
       " 'treatment',\n",
       " 'bumbling',\n",
       " 'mediocrity',\n",
       " 'needing',\n",
       " 'eyes',\n",
       " 'drum',\n",
       " 'indian',\n",
       " 'nerve',\n",
       " 'disappeared',\n",
       " 'shakespeare',\n",
       " 'kitty',\n",
       " 'caretaker',\n",
       " 'buff',\n",
       " 'motel',\n",
       " 'awry',\n",
       " 'ruled',\n",
       " 'marijuana',\n",
       " 'annoy',\n",
       " 'travels',\n",
       " 'deck',\n",
       " 'foolish',\n",
       " 'championship',\n",
       " 'funniest',\n",
       " 'acceptable',\n",
       " 'rejected',\n",
       " 'justified',\n",
       " 'kitchen',\n",
       " 'informative',\n",
       " 'claim',\n",
       " 'token',\n",
       " 'wounds',\n",
       " 'pain',\n",
       " 'erotic',\n",
       " 'purchase',\n",
       " 'stealing',\n",
       " 'grabs',\n",
       " 'communication',\n",
       " 'entertain',\n",
       " 'potter',\n",
       " 'overall',\n",
       " 'repressed',\n",
       " 'renaissance',\n",
       " 'lasted',\n",
       " 'philosophical',\n",
       " 'absurd',\n",
       " 'standout',\n",
       " 'motivations',\n",
       " 'creates',\n",
       " 'safely',\n",
       " 'security',\n",
       " 'hired',\n",
       " 'nonetheless',\n",
       " 'cinematographer',\n",
       " 'ripped',\n",
       " 'limit',\n",
       " 'rate',\n",
       " 'notion',\n",
       " 'acted',\n",
       " 'america',\n",
       " 'middle',\n",
       " 'outing',\n",
       " 'sending',\n",
       " 'sixteen',\n",
       " 'rifle',\n",
       " 'sorely',\n",
       " 'armstrong',\n",
       " 'grendel',\n",
       " 'gypsy',\n",
       " 'lucy',\n",
       " 'cyborg',\n",
       " 'ship',\n",
       " 'front',\n",
       " 'union',\n",
       " 'underrated',\n",
       " 'went',\n",
       " 'insult',\n",
       " 'correctly',\n",
       " 'antwone',\n",
       " 'kate',\n",
       " 'company',\n",
       " 'winchester',\n",
       " 'believed',\n",
       " 'runner',\n",
       " 'looking',\n",
       " 'tame',\n",
       " 'theatres',\n",
       " 'motions',\n",
       " 'promptly',\n",
       " 'orphan',\n",
       " 'ryan',\n",
       " 'baker',\n",
       " 'stomach',\n",
       " 'cg',\n",
       " 'strained',\n",
       " 'italian',\n",
       " 'whilst',\n",
       " 'approaches',\n",
       " 'european',\n",
       " 'outset',\n",
       " 'newspaper',\n",
       " 'fassbinder',\n",
       " 'deliverance',\n",
       " 'intensely',\n",
       " 'systems',\n",
       " 'swedish',\n",
       " 'fest',\n",
       " 'yokai',\n",
       " 'educational',\n",
       " 'jessica',\n",
       " 'proceedings',\n",
       " 'reverse',\n",
       " 'hunter',\n",
       " 'closest',\n",
       " 'lundgren',\n",
       " 'reader',\n",
       " 'obscurity',\n",
       " 'taking',\n",
       " 'ignores',\n",
       " 'mistakes',\n",
       " 'wins',\n",
       " 'elliott',\n",
       " 'rd',\n",
       " 'shouting',\n",
       " 'towards',\n",
       " 'blatant',\n",
       " 'deliver',\n",
       " 'river',\n",
       " 'sort',\n",
       " 'babes',\n",
       " 'buck',\n",
       " 'incident',\n",
       " 'insurance',\n",
       " 'stating',\n",
       " 'interest',\n",
       " 'stale',\n",
       " 'anything',\n",
       " 'underwear',\n",
       " 'question',\n",
       " 'brides',\n",
       " 'celluloid',\n",
       " 'belongs',\n",
       " 'planes',\n",
       " 'receive',\n",
       " 'whites',\n",
       " 'dialog',\n",
       " 'deals',\n",
       " 'wrestling',\n",
       " 'nostalgic',\n",
       " 'locations',\n",
       " 'latino',\n",
       " 'lucky',\n",
       " 'understanding',\n",
       " 'warden',\n",
       " 'award',\n",
       " 'admirer',\n",
       " 'bill',\n",
       " 'carradine',\n",
       " 'scary',\n",
       " 'framed',\n",
       " 'masters',\n",
       " 'succeed',\n",
       " 'fanatic',\n",
       " 'release',\n",
       " 'guru',\n",
       " 'fans',\n",
       " 'reality',\n",
       " 'maybe',\n",
       " 'traumatic',\n",
       " 'wound',\n",
       " 'dustin',\n",
       " 'hates',\n",
       " 'pm',\n",
       " 'dialogue',\n",
       " 'perspective',\n",
       " 'je',\n",
       " 'thus',\n",
       " 'busey',\n",
       " 'films',\n",
       " 'hurt',\n",
       " 'gathering',\n",
       " 'pitched',\n",
       " 'make',\n",
       " 'testing',\n",
       " 'elizabeth',\n",
       " 'tacky',\n",
       " 'necessity',\n",
       " 'vaughn',\n",
       " 'conflicts',\n",
       " 'jesse',\n",
       " 'possibility',\n",
       " 'marries',\n",
       " 'macho',\n",
       " 'joking',\n",
       " 'truth',\n",
       " 'break',\n",
       " 'skits',\n",
       " 'sequences',\n",
       " 'uh',\n",
       " 'clash',\n",
       " 'care',\n",
       " 'issues',\n",
       " 'prize',\n",
       " 'jean',\n",
       " 'directions',\n",
       " 'sweet',\n",
       " 'glad',\n",
       " 'gosh',\n",
       " 'paper',\n",
       " 'involvement',\n",
       " 'mitchell',\n",
       " 'monroe',\n",
       " 'sh',\n",
       " 'blend',\n",
       " 'questions',\n",
       " 'edges',\n",
       " 'competent',\n",
       " 'bulk',\n",
       " 'shown',\n",
       " 'exclusively',\n",
       " 'double',\n",
       " 'begin',\n",
       " 'exorcist',\n",
       " 'patient',\n",
       " 'theories',\n",
       " 'tales',\n",
       " 'bomb',\n",
       " 'zero',\n",
       " 'programs',\n",
       " 'come',\n",
       " 'cruise',\n",
       " 'herd',\n",
       " 'radio',\n",
       " 'ostensibly',\n",
       " 'tube',\n",
       " 'nine',\n",
       " 'anticipated',\n",
       " 'society',\n",
       " 'flows',\n",
       " 'consistent',\n",
       " 'slasher',\n",
       " 'losing',\n",
       " 'cut',\n",
       " 'realize',\n",
       " 'appeal',\n",
       " 'spends',\n",
       " 'leigh',\n",
       " 'semi',\n",
       " 'clothes',\n",
       " 'todd',\n",
       " 'followers',\n",
       " 'oliver',\n",
       " 'sooner',\n",
       " 'inappropriate',\n",
       " 'navy',\n",
       " 'curly',\n",
       " 'imagining',\n",
       " 'suddenly',\n",
       " 'welles',\n",
       " 'chamber',\n",
       " 'plight',\n",
       " 'unsettling',\n",
       " 'therefore',\n",
       " 'coffee',\n",
       " 'backwards',\n",
       " 'wayans',\n",
       " 'lena',\n",
       " 'kathy',\n",
       " 'bang',\n",
       " 'slowly',\n",
       " 'disastrous',\n",
       " 'memorable',\n",
       " 'figured',\n",
       " 'continues',\n",
       " 'struggling',\n",
       " 'assassin',\n",
       " 'rhythm',\n",
       " 'kyle',\n",
       " 'major',\n",
       " 'potentially',\n",
       " 'suitable',\n",
       " 'funny',\n",
       " 'acting',\n",
       " 'examples',\n",
       " 'attract',\n",
       " 'shares',\n",
       " 'lower',\n",
       " 'joker',\n",
       " 'girls',\n",
       " 'fatal',\n",
       " 'angela',\n",
       " 'voyage',\n",
       " 'regards',\n",
       " 'spelling',\n",
       " 'minimal',\n",
       " 'astaire',\n",
       " 'forward',\n",
       " 'lola',\n",
       " 'republic',\n",
       " 'drug',\n",
       " 'originality',\n",
       " 'seemed',\n",
       " 'ruined',\n",
       " 'jones',\n",
       " 'hunt',\n",
       " 'states',\n",
       " 'conclude',\n",
       " 'epitome',\n",
       " 'godard',\n",
       " 'uncomfortable',\n",
       " 'lackluster',\n",
       " 'vcr',\n",
       " 'wherever',\n",
       " 'dolls',\n",
       " 'meandering',\n",
       " 'crimes',\n",
       " 'chick',\n",
       " 'phenomenon',\n",
       " 'cell',\n",
       " 'morse',\n",
       " 'learn',\n",
       " 'selection',\n",
       " 'femme',\n",
       " 'higher',\n",
       " 'west',\n",
       " 'mary',\n",
       " 'wonderful',\n",
       " 'legal',\n",
       " 'away',\n",
       " 'detail',\n",
       " 'blond',\n",
       " 'phillip',\n",
       " 'motion',\n",
       " 'spirit',\n",
       " 'kolchak',\n",
       " 'lying',\n",
       " 'krueger',\n",
       " 'canadian',\n",
       " 'participants',\n",
       " 'nominated',\n",
       " 'liberty',\n",
       " 'sticking',\n",
       " 'playwright',\n",
       " 'intrigue',\n",
       " 'wwii',\n",
       " 'behavior',\n",
       " 'gilliam',\n",
       " 'beings',\n",
       " 'forgiveness',\n",
       " 'thrilling',\n",
       " 'guessing',\n",
       " 'altman',\n",
       " 'stereotyped',\n",
       " 'quintessential',\n",
       " 'galactica',\n",
       " 'glimpse',\n",
       " 'sinking',\n",
       " 'cross',\n",
       " 'cool',\n",
       " 'pan',\n",
       " 'fashion',\n",
       " 'meaningless',\n",
       " 'espionage',\n",
       " 'japanese',\n",
       " 'vulnerable',\n",
       " 'portrayed',\n",
       " 'thin',\n",
       " 'matthew',\n",
       " 'represents',\n",
       " 'brilliance',\n",
       " 'sadness',\n",
       " 'redeem',\n",
       " 'team',\n",
       " 'blowing',\n",
       " 'looked',\n",
       " 'bye',\n",
       " 'awards',\n",
       " 'lazy',\n",
       " 'tolerate',\n",
       " 'richard',\n",
       " 'servant',\n",
       " 'messing',\n",
       " 'visual',\n",
       " 'beliefs',\n",
       " 'satan',\n",
       " 'triple',\n",
       " 'photographer',\n",
       " 'lindsay',\n",
       " 'goldie',\n",
       " 'esquire',\n",
       " 'christopher',\n",
       " 'terrific',\n",
       " 'murderer',\n",
       " 'macdonald',\n",
       " 'tune',\n",
       " 'jews',\n",
       " 'wider',\n",
       " 'equal',\n",
       " 'painter',\n",
       " 'carrey',\n",
       " 'goers',\n",
       " 'pick',\n",
       " 'verbal',\n",
       " 'stoned',\n",
       " 'love',\n",
       " 'subplot',\n",
       " 'gene',\n",
       " 'stooges',\n",
       " 'shining',\n",
       " 'lois',\n",
       " 'psychologist',\n",
       " 'elevator',\n",
       " 'convince',\n",
       " 'gentleman',\n",
       " 'protect',\n",
       " 'exposition',\n",
       " 'complexity',\n",
       " 'arrow',\n",
       " 'pitch',\n",
       " 'montage',\n",
       " 'numbers',\n",
       " 'kidnapping',\n",
       " 'coffin',\n",
       " 'auteur',\n",
       " 'pacino',\n",
       " 'haired',\n",
       " 'uniformly',\n",
       " 'revelation',\n",
       " 'secretly',\n",
       " 'enigmatic',\n",
       " 'glenda',\n",
       " 'sleeping',\n",
       " 'blue',\n",
       " 'hopeless',\n",
       " 'slimy',\n",
       " 'verge',\n",
       " 'bugs',\n",
       " 'beckham',\n",
       " 'advice',\n",
       " 'walter',\n",
       " 'rush',\n",
       " 'innocence',\n",
       " 'detailed',\n",
       " 'switch',\n",
       " 'much',\n",
       " 'emma',\n",
       " 'newcomer',\n",
       " 'expects',\n",
       " 'lord',\n",
       " 'drags',\n",
       " 'ww',\n",
       " 'handed',\n",
       " 'assistance',\n",
       " 'mexican',\n",
       " 'ultimately',\n",
       " 'bike',\n",
       " 'settled',\n",
       " 'primarily',\n",
       " 'opens',\n",
       " 'dream',\n",
       " 'recommended',\n",
       " 'faster',\n",
       " 'ex',\n",
       " 'eleven',\n",
       " 'buffy',\n",
       " 'resolution',\n",
       " 'serving',\n",
       " 'miami',\n",
       " 'harrowing',\n",
       " 'stallone',\n",
       " 'nazi',\n",
       " 'porter',\n",
       " 'unfolds',\n",
       " 'rainy',\n",
       " 'herman',\n",
       " 'bully',\n",
       " 'motivation',\n",
       " 'admittedly',\n",
       " 'hadley',\n",
       " 'slide',\n",
       " 'town',\n",
       " 'understand',\n",
       " 'frame',\n",
       " 'mannered',\n",
       " 'fathers',\n",
       " 'blake',\n",
       " 'evolution',\n",
       " 'help',\n",
       " 'eventually',\n",
       " 'manner',\n",
       " 'case',\n",
       " 'moved',\n",
       " 'bridget',\n",
       " 'character',\n",
       " 'pigs',\n",
       " 'brand',\n",
       " 'strange',\n",
       " 'disease',\n",
       " 'dirt',\n",
       " 'helicopter',\n",
       " 'average',\n",
       " 'strongest',\n",
       " 'actor',\n",
       " 'rendered',\n",
       " 'seen',\n",
       " 'parrot',\n",
       " 'macy',\n",
       " 'unfortunately',\n",
       " 'hatred',\n",
       " 'iran',\n",
       " 'approached',\n",
       " 'spock',\n",
       " 'sees',\n",
       " 'employee',\n",
       " 'quinn',\n",
       " 'hits',\n",
       " 'senseless',\n",
       " 'discovery',\n",
       " 'les',\n",
       " 'outfit',\n",
       " 'ocean',\n",
       " 'fiend',\n",
       " 'realm',\n",
       " 'graduate',\n",
       " 'unusually',\n",
       " 'sub',\n",
       " 'stalking',\n",
       " 'marks',\n",
       " 'grade',\n",
       " 'selfish',\n",
       " 'intense',\n",
       " 'cancer',\n",
       " 'amy',\n",
       " 'vampire',\n",
       " 'slugs',\n",
       " 'razor',\n",
       " 'commercial',\n",
       " 'ruby',\n",
       " 'gus',\n",
       " 'red',\n",
       " 'hometown',\n",
       " 'costume',\n",
       " 'right',\n",
       " 'planet',\n",
       " 'medium',\n",
       " 'explain',\n",
       " 'crime',\n",
       " 'andrews',\n",
       " 'mathieu',\n",
       " 'sink',\n",
       " 'frustrating',\n",
       " 'inept',\n",
       " 'illogical',\n",
       " 'cab',\n",
       " 'bites',\n",
       " 'harlow',\n",
       " 'scrooge',\n",
       " 'malden',\n",
       " 'beg',\n",
       " 'interested',\n",
       " 'due',\n",
       " 'performers',\n",
       " 'informs',\n",
       " 'racing',\n",
       " 'journey',\n",
       " 'brady',\n",
       " 'joins',\n",
       " 'late',\n",
       " 'paint',\n",
       " 'hulk',\n",
       " 'battlestar',\n",
       " 'subtle',\n",
       " 'jo',\n",
       " 'workers',\n",
       " 'arriving',\n",
       " 'eli',\n",
       " 'horny',\n",
       " 'amateur',\n",
       " 'rational',\n",
       " 'chuckles',\n",
       " 'profession',\n",
       " 'trial',\n",
       " 'burns',\n",
       " 'sensitive',\n",
       " 'seasoned',\n",
       " 'pressed',\n",
       " 'famous',\n",
       " 'agenda',\n",
       " 'puts',\n",
       " 'danish',\n",
       " 'wish',\n",
       " 'corey',\n",
       " 'divine',\n",
       " 'exchange',\n",
       " 'rushed',\n",
       " 'courageous',\n",
       " 'jay',\n",
       " 'beautiful',\n",
       " 'location',\n",
       " 'understood',\n",
       " 'wonders',\n",
       " 'limited',\n",
       " 'lie',\n",
       " 'gift',\n",
       " 'dose',\n",
       " 'guevara',\n",
       " 'visually',\n",
       " 'credited',\n",
       " 'tour',\n",
       " 'questionable',\n",
       " 'escape',\n",
       " 'devil',\n",
       " 'inaccuracies',\n",
       " 'witnesses',\n",
       " 'ultimatum',\n",
       " 'ego',\n",
       " 'crying',\n",
       " 'mandy',\n",
       " 'hidden',\n",
       " 'gasp',\n",
       " 'amitabh',\n",
       " 'aspects',\n",
       " 'idiot',\n",
       " 'shoulder',\n",
       " 'weaker',\n",
       " 'lugosi',\n",
       " 'pair',\n",
       " 'curtain',\n",
       " 'coburn',\n",
       " 'week',\n",
       " 'coup',\n",
       " 'using',\n",
       " 'scared',\n",
       " 'crew',\n",
       " 'channel',\n",
       " 'animation',\n",
       " 'collette',\n",
       " 'atmospheric',\n",
       " 'musicians',\n",
       " 'heard',\n",
       " 'corn',\n",
       " 'simple',\n",
       " 'eternal',\n",
       " 'chopped',\n",
       " 'linda',\n",
       " 'playing',\n",
       " 'nobody',\n",
       " 'enchanted',\n",
       " 'donna',\n",
       " 'jokes',\n",
       " 'shots',\n",
       " 'cape',\n",
       " 'teens',\n",
       " 'sports',\n",
       " 'oscars',\n",
       " 'gothic',\n",
       " 'nudity',\n",
       " 'lighthearted',\n",
       " 'stewart',\n",
       " 'shake',\n",
       " 'keys',\n",
       " 'bo',\n",
       " 'bits',\n",
       " 'pulled',\n",
       " 'depressed',\n",
       " 'foot',\n",
       " 'scenes',\n",
       " 'bought',\n",
       " 'repeated',\n",
       " 'april',\n",
       " 'programming',\n",
       " 'carl',\n",
       " 'dimensional',\n",
       " 'restaurant',\n",
       " 'implies',\n",
       " 'overblown',\n",
       " 'scored',\n",
       " 'flick',\n",
       " 'stroke',\n",
       " 'stabbing',\n",
       " 'economy',\n",
       " 'interrupted',\n",
       " 'filmed',\n",
       " 'lake',\n",
       " 'godfather',\n",
       " 'faults',\n",
       " 'ought',\n",
       " 'pattern',\n",
       " 'rightly',\n",
       " 'brick',\n",
       " 'facing',\n",
       " 'grandpa',\n",
       " 'universal',\n",
       " 'educated',\n",
       " 'sunset',\n",
       " 'downs',\n",
       " 'rohmer',\n",
       " 'dean',\n",
       " 'halloween',\n",
       " 'ahead',\n",
       " 'techniques',\n",
       " 'drinks',\n",
       " 'demonstrated',\n",
       " 'mysteries',\n",
       " 'heartwarming',\n",
       " 'nuances',\n",
       " 'footage',\n",
       " 'launch',\n",
       " 'discussed',\n",
       " 'pray',\n",
       " 'bed',\n",
       " 'contain',\n",
       " 'seek',\n",
       " 'precious',\n",
       " 'conveyed',\n",
       " 'laurie',\n",
       " 'dysfunctional',\n",
       " 'infected',\n",
       " 'daniels',\n",
       " 'wondering',\n",
       " 'hundreds',\n",
       " 'cons',\n",
       " 'unlike',\n",
       " 'sheen',\n",
       " 'incidental',\n",
       " 'crocodile',\n",
       " 'inaccurate',\n",
       " 'creations',\n",
       " 'bridges',\n",
       " 'rips',\n",
       " 'sandy',\n",
       " 'assistant',\n",
       " 'frankly',\n",
       " 'grew',\n",
       " 'marcus',\n",
       " 'act',\n",
       " 'edmund',\n",
       " 'maintain',\n",
       " 'portrayals',\n",
       " 'unable',\n",
       " 'musician',\n",
       " 'partners',\n",
       " 'retro',\n",
       " 'qualify',\n",
       " 'tone',\n",
       " 'exceptionally',\n",
       " 'meantime',\n",
       " 'alas',\n",
       " 'abrupt',\n",
       " 'pairing',\n",
       " 'drunk',\n",
       " 'weekend',\n",
       " 'uncut',\n",
       " 'conductor',\n",
       " 'delightfully',\n",
       " 'august',\n",
       " 'kurt',\n",
       " 'distracting',\n",
       " 'mira',\n",
       " 'accounts',\n",
       " 'abomination',\n",
       " 'impressive',\n",
       " 'futuristic',\n",
       " 'controversy',\n",
       " 'said',\n",
       " 'flashbacks',\n",
       " 'unexpectedly',\n",
       " 'crazy',\n",
       " 'featured',\n",
       " 'brendan',\n",
       " 'festivals',\n",
       " 'class',\n",
       " 'loretta',\n",
       " 'wayne',\n",
       " 'narrow',\n",
       " 'first',\n",
       " 'wu',\n",
       " 'suspend',\n",
       " 'seldom',\n",
       " 'darn',\n",
       " 'mcqueen',\n",
       " 'keaton',\n",
       " 'barnes',\n",
       " 'naturally',\n",
       " 'premiere',\n",
       " 'gillian',\n",
       " 'boris',\n",
       " 'strike',\n",
       " 'invasion',\n",
       " 'absent',\n",
       " 'promote',\n",
       " 'burning',\n",
       " 'quasi',\n",
       " 'hundred',\n",
       " 'quickly',\n",
       " 'unfamiliar',\n",
       " 'hugh',\n",
       " 'cage',\n",
       " 'conventional',\n",
       " 'influence',\n",
       " 'fright',\n",
       " 'belt',\n",
       " 'fbi',\n",
       " 'angie',\n",
       " 'connie',\n",
       " 'bloodbath',\n",
       " 'whats',\n",
       " 'graham',\n",
       " 'moss',\n",
       " 'episodes',\n",
       " 'firm',\n",
       " 'happy',\n",
       " 'aided',\n",
       " 'tiny',\n",
       " 'drivel',\n",
       " 'dropping',\n",
       " 'adaptations',\n",
       " 'deserve',\n",
       " 'maurice',\n",
       " 'transplant',\n",
       " 'sydney',\n",
       " 'bunny',\n",
       " 'tied',\n",
       " 'unreal',\n",
       " 'europeans',\n",
       " 'racism',\n",
       " 'visuals',\n",
       " 'bringing',\n",
       " 'row',\n",
       " 'new',\n",
       " 'range',\n",
       " 'tones',\n",
       " 'fuzzy',\n",
       " 'lowe',\n",
       " 'spencer',\n",
       " 'tries',\n",
       " 'images',\n",
       " 'cousin',\n",
       " 'spies',\n",
       " 'confusion',\n",
       " 'alone',\n",
       " 'article',\n",
       " 'importance',\n",
       " 'continue',\n",
       " 'terror',\n",
       " 'reward',\n",
       " 'rabbit',\n",
       " 'forty',\n",
       " 'referred',\n",
       " 'arguably',\n",
       " 'direction',\n",
       " 'clip',\n",
       " 'empty',\n",
       " 'upbeat',\n",
       " 'impressed',\n",
       " 'talked',\n",
       " 'mall',\n",
       " 'justin',\n",
       " 'madsen',\n",
       " 'meaning',\n",
       " 'passed',\n",
       " 'fictional',\n",
       " 'profound',\n",
       " 'hughes',\n",
       " 'pro',\n",
       " 'exploit',\n",
       " 'tragic',\n",
       " 'stanwyck',\n",
       " 'incapable',\n",
       " 'vampires',\n",
       " 'noticeable',\n",
       " 'achieves',\n",
       " 'lukas',\n",
       " 'division',\n",
       " 'soprano',\n",
       " ...}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(model.wv.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44ed9f6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.99248832e-01, -3.63430679e-01, -1.11796208e-01, -2.10639611e-01,\n",
       "       -3.92391384e-01,  1.05572367e+00,  6.31620968e-03, -5.81072569e-01,\n",
       "       -7.35513389e-01,  1.17671025e+00,  4.03569818e-01,  9.35961068e-01,\n",
       "       -5.84544301e-01, -2.96590596e-01,  4.66720253e-01, -7.94575274e-01,\n",
       "        7.73075402e-01,  6.72431439e-02, -4.16245192e-01,  6.61186948e-02,\n",
       "        1.38294053e+00,  3.99340242e-01,  8.05702284e-02, -5.28923810e-01,\n",
       "       -1.50466397e-01, -3.74172539e-01,  1.82456821e-01,  1.58939720e-03,\n",
       "       -7.93863952e-01, -3.48301381e-01, -2.81143218e-01,  6.85884476e-01,\n",
       "        1.24131858e-01,  5.02883434e-01,  4.12299305e-01, -1.20862179e-01,\n",
       "       -1.72999173e-01,  1.58614233e-01, -1.62101477e-01, -6.11127377e-01,\n",
       "       -1.60749689e-01, -6.14646077e-01,  3.51361960e-01,  6.17551267e-01,\n",
       "       -2.86332190e-01,  1.99311048e-01,  2.85046875e-01,  5.20428538e-01,\n",
       "       -5.88844657e-01,  4.37829167e-01,  1.27428576e-01,  4.06116217e-01,\n",
       "        8.69752586e-01, -2.72399504e-02, -2.75829732e-01, -5.50254434e-02,\n",
       "        5.26309073e-01,  6.89488351e-01, -3.47159833e-01,  2.44631097e-01,\n",
       "       -2.46903226e-01, -2.24776074e-01,  5.66132247e-01, -9.88696337e-01,\n",
       "       -5.57436287e-01,  4.37552243e-01, -3.66636574e-01,  5.32961726e-01,\n",
       "       -8.21600139e-01, -3.68960738e-01,  4.68259424e-01,  5.38285494e-01,\n",
       "       -8.51566046e-02,  2.51067966e-01,  3.97249520e-01, -9.76733327e-01,\n",
       "        3.64339739e-01,  6.16593301e-01,  5.84385872e-01, -4.74507600e-01,\n",
       "       -3.93047839e-01,  5.01303792e-01,  5.48176348e-01,  2.41663978e-01,\n",
       "        5.46297491e-01, -7.60093927e-01,  7.56766558e-01, -4.75716084e-01,\n",
       "        1.31533608e-01, -1.30581260e-01,  4.36585397e-03,  3.72589767e-01,\n",
       "        1.19722478e-01,  1.12283297e-01,  4.97188509e-01,  3.97345901e-01,\n",
       "       -3.58816713e-01,  5.26676178e-01, -7.18063474e-01,  5.44310883e-02,\n",
       "        2.97424316e-01,  2.53780842e-01, -7.21447825e-01,  5.26466668e-01,\n",
       "        3.90636772e-01, -1.63772255e-01,  2.83860117e-01, -7.18549430e-01,\n",
       "        1.85530528e-01, -2.33798958e-02, -3.84263247e-01,  5.44809043e-01,\n",
       "       -1.75134614e-01,  8.63629878e-02, -5.39532900e-01,  7.39678919e-01,\n",
       "        7.25937366e-01, -9.26976278e-02, -1.10810973e-01, -2.08537966e-01,\n",
       "       -1.26980036e-01,  5.27010620e-01, -2.36564130e-01, -2.41517708e-01,\n",
       "        6.30653858e-01,  3.32089245e-01, -1.36831820e+00, -6.37727797e-01,\n",
       "        8.11792985e-02,  1.01223433e+00,  5.68006694e-01,  1.10452986e+00,\n",
       "       -4.02659699e-02, -5.56989908e-01,  1.39719129e-01,  3.12366504e-02,\n",
       "       -4.62466359e-01,  9.06891704e-01,  1.94581524e-02, -6.94862306e-01,\n",
       "       -2.52641737e-01, -5.35122693e-01,  1.42506853e-01, -3.58557999e-01,\n",
       "        3.02355379e-01, -8.52437168e-02, -7.60831013e-02, -2.11618707e-01,\n",
       "       -9.41314280e-01,  7.90915668e-01, -2.19327375e-01, -2.57652432e-01,\n",
       "       -2.62424082e-01,  1.19935906e+00,  4.46243167e-01,  8.52061331e-01,\n",
       "        4.21784043e-01,  9.53185558e-02, -9.92643926e-03,  3.91349077e-01,\n",
       "        1.80068031e-01, -4.38176304e-01, -7.15836957e-02, -6.26389802e-01,\n",
       "       -8.15409496e-02, -6.37545168e-01, -9.44631770e-02, -5.08836448e-01,\n",
       "       -1.49486125e-01, -4.69000220e-01,  4.00564931e-02,  2.85813510e-01,\n",
       "        4.41811323e-01,  4.52997744e-01,  4.28187847e-01, -3.84043336e-01,\n",
       "       -2.58125216e-01, -6.94432437e-01, -1.59989044e-01,  1.10675439e-01,\n",
       "       -5.22239983e-01,  6.38345957e-01,  1.82327613e-01,  5.52296460e-01,\n",
       "       -4.21019286e-01, -5.56290783e-02,  4.36803281e-01,  3.80696148e-01,\n",
       "        8.82083416e-01, -3.79757196e-01,  6.72435939e-01,  5.85876226e-01,\n",
       "       -8.60229313e-01,  2.57997692e-01,  4.42250401e-01,  1.48536772e-01,\n",
       "       -9.42906216e-02,  1.60611235e-02,  4.49404627e-01,  4.66244400e-01,\n",
       "       -8.00140977e-01, -5.62881492e-02,  3.71216029e-01, -3.41976702e-01,\n",
       "        9.39806163e-01, -1.18124232e-01,  1.42693490e-01,  7.77891695e-01,\n",
       "       -1.62968829e-01,  6.40576482e-01, -4.77795273e-01, -7.83925772e-01,\n",
       "        6.22231424e-01,  2.28509113e-01, -7.32807100e-01, -8.60389620e-02,\n",
       "        2.24205106e-01, -1.52356952e-01, -1.47028908e-01, -5.35059214e-01,\n",
       "       -2.15041131e-01, -2.25205403e-02, -5.73201358e-01, -1.03242362e+00,\n",
       "       -7.29067549e-02,  3.98719966e-01,  7.66474307e-01,  3.61241519e-01,\n",
       "       -5.45316219e-01,  2.74727255e-01, -7.09957957e-01, -9.92536694e-02,\n",
       "       -8.36599112e-01,  4.14739996e-02, -7.65521005e-02,  7.96391629e-03,\n",
       "       -1.90037400e-01,  3.08107644e-01,  7.62069404e-01, -6.61943853e-01,\n",
       "        1.59823261e-02,  9.52508375e-02,  8.21704805e-01,  8.00108850e-01,\n",
       "        6.98152363e-01, -3.01771879e-01,  5.41790187e-01, -1.29981667e-01,\n",
       "       -3.47934157e-01, -1.54689223e-01,  2.41056725e-01,  6.77321374e-01,\n",
       "       -5.42923391e-01, -5.32822132e-01, -8.98374736e-01, -7.83971369e-01,\n",
       "        7.75432467e-01,  1.51746601e-01,  5.59571803e-01,  3.02926540e-01,\n",
       "       -1.04594529e-01, -6.98207438e-01,  6.33821487e-01,  5.35461307e-01,\n",
       "        6.86347425e-01,  3.34917217e-01,  8.72116983e-01,  2.79148728e-01,\n",
       "       -3.62004966e-01, -2.47687444e-01,  1.72783300e-01, -4.49912459e-01,\n",
       "       -1.10872746e-01,  3.96418589e-04,  1.35479242e-01,  9.25118446e-01,\n",
       "        3.19907308e-01, -1.23937383e-01, -3.85926276e-01, -2.44842321e-01,\n",
       "       -2.58732408e-01, -5.11545956e-01,  3.91326159e-01,  5.28616130e-01,\n",
       "        3.20763171e-01,  9.08625484e-01,  5.33298373e-01, -2.79068828e-01,\n",
       "        3.48583050e-02, -3.24444026e-01, -2.42095456e-01,  3.07868961e-02,\n",
       "       -7.50579357e-01,  1.04266591e-02,  1.02710210e-01,  5.25135338e-01,\n",
       "        3.06615066e-02,  2.91189849e-01,  2.22422168e-01, -1.07153252e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어 'stuff'의 word2vec 임베딩 결과 벡터 예시\n",
    "\n",
    "model.wv['stuff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d461acd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.wv['stuff'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df7c5202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.12239955,  0.15240708, -0.16109604, ...,  0.08871711,\n",
       "        -0.16920629, -0.01497271],\n",
       "       [ 0.09853074,  0.00749504, -0.17417702, ...,  0.21652001,\n",
       "         0.13724329, -0.03892314],\n",
       "       [-0.03684576,  0.1319762 , -0.0588348 , ..., -0.12680957,\n",
       "        -0.01914953, -0.05520926],\n",
       "       ...,\n",
       "       [-0.01455216,  0.2316143 , -0.17487827, ...,  0.26816922,\n",
       "         0.01325628,  0.03573281],\n",
       "       [ 0.19930726,  0.16365069, -0.17181775, ...,  0.12439137,\n",
       "         0.07807317,  0.00732926],\n",
       "       [-0.01111315,  0.3054053 , -0.00737039, ..., -0.00944764,\n",
       "         0.13471833, -0.06522714]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e6c265c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 300)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "017784c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "x = test_data_vecs\n",
    "y = np.array(sentiments)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x,y,test_size=TEST_SPLIT, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c5a59bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.866400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\answl\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lgs = LogisticRegression(class_weight='balanced')\n",
    "lgs.fit(x_train, y_train)\n",
    "\n",
    "print(\"Accuracy: %f\" % lgs.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5afc892a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_CLEAN_DATA = 'test_clean.csv'\n",
    "test_data = pd.read_csv(DATA_IN_PATH + TEST_CLEAN_DATA)\n",
    "test_review = list(test_data['review'])\n",
    "\n",
    "test_sentences = list()\n",
    "for review in test_review:\n",
    "    test_sentences.append(review.split())\n",
    "    \n",
    "test_data_vecs = get_dataset(test_sentences, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e7140eae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "59a84b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-08 14:04:47,429 : INFO : Word2Vec lifecycle event {'fname_or_handle': '300features_40minwords_10context', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-04-08T14:04:47.429438', 'gensim': '4.1.2', 'python': '3.7.13 (default, Mar 28 2022, 08:03:21) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'saving'}\n",
      "2023-04-08 14:04:47,430 : INFO : not storing attribute cum_table\n",
      "2023-04-08 14:04:47,451 : INFO : saved 300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "DATA_OUT_PATH = './data_out/'\n",
    "\n",
    "test_predicted = lgs.predict(test_data_vecs)\n",
    "\n",
    "ids = list(test_data['id'])\n",
    "answer_dataset = pd.DataFrame({'id':ids, 'sentiment':test_predicted})\n",
    "answer_dataset.to_csv(DATA_OUT_PATH + 'lgs_w2v_answer.csv',\n",
    "                     index=False, quoting=3)\n",
    "\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8beb584",
   "metadata": {},
   "source": [
    "모델 저장 시 모델 이름에 하이퍼파라미터 설정 내용 담기 ! \n",
    "\n",
    "Word2Vec.load()를 통해 나중에 다시 모델 사용 가능!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9642da2",
   "metadata": {},
   "source": [
    "캐글 제출 --> 정확도 0.86104 \n",
    "\n",
    "--> 난 책과 달리 tfidf (char 기준) 보다 0.01 높게 나옴 !\n",
    "\n",
    "- word2vec이 항상 좋은 결과를 만들지는 않음 \n",
    "    - 데이터가 더 많을 경우 word2vec 이 보통 더 좋은 결과"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
