{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cd37052",
   "metadata": {},
   "source": [
    "!pip install gluonnlp==0.9.1\n",
    "\n",
    "!pip install sentencepiece==0.1.85\n",
    "- 해당버전으로 설치하지 않으면 tokenizer() 실행 시, nbest_size 런타임에러 남!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acf98e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "\n",
    "import gluonnlp as nlp\n",
    "from gluonnlp.data import SentencepieceTokenizer\n",
    "from transformers import TFGPT2LMHeadModel\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "456bdc8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the model checkpoint at ./gpt_ckpt.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "class GPT2Model(tf.keras.Model):\n",
    "    def __init__(self, dir_path):\n",
    "        super(GPT2Model, self).__init__()\n",
    "        self.gpt2 = TFGPT2LMHeadModel.from_pretrained(dir_path)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return self.gpt2(inputs)[0]\n",
    "    \n",
    "BASE_MODEL_PATH = './gpt_ckpt'\n",
    "gpt_model = GPT2Model(BASE_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab0d85d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.9.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9bab888",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 10\n",
    "MAX_LEN = 30\n",
    "TOKENIZER_PATH = './gpt_ckpt/gpt2_kor_tokenizer.spiece'\n",
    "\n",
    "# from gluonnlp.data import SentencepieceTokenizer\n",
    "tokenizer = SentencepieceTokenizer(TOKENIZER_PATH)\n",
    "#                                   num_best=0, # 디폴트\n",
    "#                                   alpha=1.0) # 디폴트\n",
    "\"\"\"\n",
    "num_best(int): A scalar for sampling subwords.\n",
    "- 0,1: 샘플링 미실시\n",
    "- 2 이상: 샘플링 실시 \n",
    "- -1 이하: 모든 후보에서 샘플링\n",
    "alpha(float): A scalar for a smoothing parameter.\n",
    "\"\"\"\n",
    "\n",
    "# import gluonnlp as nlp\n",
    "vocab = nlp.vocab.BERTVocab.from_sentencepiece(TOKENIZER_PATH,\n",
    "                                              mask_token=None,\n",
    "                                              sep_token=None,\n",
    "                                              cls_token=None,\n",
    "                                              unknown_token='<unk>',\n",
    "                                              padding_token='<pad>',\n",
    "                                              bos_token='<s>',\n",
    "                                              eos_token='</s>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "014f01d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28911"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab['안녕']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a920051",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3918"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab['▁나는']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e55cbc8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁안녕하세요', '▁나는', '▁사람', '입니다', '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = \"안녕하세요 나는 사람입니다.\"\n",
    "b = tokenizer(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa1a74cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'gluonnlp.vocab' from 'C:\\\\Users\\\\answl\\\\anaconda3\\\\envs\\\\nlp\\\\lib\\\\site-packages\\\\gluonnlp\\\\vocab\\\\__init__.py'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aca1945c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'gluonnlp.vocab' from 'C:\\\\Users\\\\answl\\\\anaconda3\\\\envs\\\\nlp\\\\lib\\\\site-packages\\\\gluonnlp\\\\vocab\\\\__init__.py'>\n"
     ]
    }
   ],
   "source": [
    "print(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eede04dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package gluonnlp.vocab in gluonnlp:\n",
      "\n",
      "NAME\n",
      "    gluonnlp.vocab - Vocabulary.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    bert\n",
      "    elmo\n",
      "    subwords\n",
      "    vocab\n",
      "\n",
      "CLASSES\n",
      "    builtins.object\n",
      "        gluonnlp.vocab.elmo.ELMoCharVocab\n",
      "        gluonnlp.vocab.subwords.SubwordFunction\n",
      "            gluonnlp.vocab.subwords.ByteSubwords\n",
      "            gluonnlp.vocab.subwords.NGramHashes\n",
      "        gluonnlp.vocab.vocab.Vocab\n",
      "            gluonnlp.vocab.bert.BERTVocab\n",
      "    \n",
      "    class BERTVocab(gluonnlp.vocab.vocab.Vocab)\n",
      "     |  BERTVocab(counter=None, max_size=None, min_freq=1, unknown_token='[UNK]', padding_token='[PAD]', bos_token=None, eos_token=None, mask_token='[MASK]', sep_token='[SEP]', cls_token='[CLS]', reserved_tokens=None, token_to_idx=None)\n",
      "     |  \n",
      "     |  Specialization of gluonnlp.Vocab for BERT models.\n",
      "     |  \n",
      "     |  BERTVocab changes default token representations of unknown and other\n",
      "     |  special tokens of gluonnlp.Vocab and adds convenience parameters to specify\n",
      "     |  mask, sep and cls tokens typically used by Bert models.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  counter : Counter or None, default None\n",
      "     |      Counts text token frequencies in the text data. Its keys will be indexed according to\n",
      "     |      frequency thresholds such as `max_size` and `min_freq`. Keys of `counter`,\n",
      "     |      `unknown_token`, and values of `reserved_tokens` must be of the same hashable type.\n",
      "     |      Examples: str, int, and tuple.\n",
      "     |  max_size : None or int, default None\n",
      "     |      The maximum possible number of the most frequent tokens in the keys of `counter` that can be\n",
      "     |      indexed. Note that this argument does not count any token from `reserved_tokens`. Suppose\n",
      "     |      that there are different keys of `counter` whose frequency are the same, if indexing all of\n",
      "     |      them will exceed this argument value, such keys will be indexed one by one according to\n",
      "     |      their __cmp__() order until the frequency threshold is met. If this argument is None or\n",
      "     |      larger than its largest possible value restricted by `counter` and `reserved_tokens`, this\n",
      "     |      argument has no effect.\n",
      "     |  min_freq : int, default 1\n",
      "     |      The minimum frequency required for a token in the keys of `counter` to be indexed.\n",
      "     |  unknown_token : hashable object or None, default '[UNK]'\n",
      "     |      The representation for any unknown token. In other words, any unknown token will be indexed\n",
      "     |      as the same representation. If None, looking up an unknown token will result in KeyError.\n",
      "     |  padding_token : hashable object or None, default '[PAD]'\n",
      "     |      The representation for the special token of padding token.\n",
      "     |  bos_token : hashable object or None, default None\n",
      "     |      The representation for the special token of beginning-of-sequence token.\n",
      "     |  eos_token : hashable object or None, default None\n",
      "     |      The representation for the special token of end-of-sequence token.\n",
      "     |  mask_token : hashable object or None, default '[MASK]'\n",
      "     |      The representation for the special token of mask token for BERT.\n",
      "     |  sep_token : hashable object or None, default '[SEP]'\n",
      "     |      A token used to separate sentence pairs for BERT.\n",
      "     |  cls_token : hashable object or None, default '[CLS]'\n",
      "     |      Classification symbol for BERT.\n",
      "     |  reserved_tokens : list of hashable objects or None, default None\n",
      "     |      A list specifying additional tokens to be added to the vocabulary.\n",
      "     |      `reserved_tokens` cannot contain `unknown_token` or duplicate reserved\n",
      "     |      tokens.\n",
      "     |      Keys of `counter`, `unknown_token`, and values of `reserved_tokens`\n",
      "     |      must be of the same hashable type. Examples of hashable types are str,\n",
      "     |      int, and tuple.\n",
      "     |  token_to_idx : dict mapping tokens (hashable objects) to int or None, default None\n",
      "     |      Optionally specifies the indices of tokens to be used by the\n",
      "     |      vocabulary. Each token in `token_to_index` must be part of the Vocab\n",
      "     |      and each index can only be associated with a single token.\n",
      "     |      `token_to_idx` is not required to contain a mapping for all tokens. For\n",
      "     |      example, it is valid to only set the `unknown_token` index to 10 (instead\n",
      "     |      of the default of 0) with `token_to_idx = {'<unk>': 10}`.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  embedding : instance of :class:`gluonnlp.embedding.TokenEmbedding`\n",
      "     |      The embedding of the indexed tokens.\n",
      "     |  idx_to_token : list of strs\n",
      "     |      A list of indexed tokens where the list indices and the token indices are aligned.\n",
      "     |  reserved_tokens : list of strs or None\n",
      "     |      A list of reserved tokens that will always be indexed.\n",
      "     |  token_to_idx : dict mapping str to int\n",
      "     |      A dict mapping each token to its index integer.\n",
      "     |  unknown_token : hashable object or None, default '[UNK]'\n",
      "     |      The representation for any unknown token. In other words, any unknown token will be indexed\n",
      "     |      as the same representation.\n",
      "     |  padding_token : hashable object or None, default '[PAD]'\n",
      "     |      The representation for padding token.\n",
      "     |  bos_token : hashable object or None, default None\n",
      "     |      The representation for beginning-of-sentence token.\n",
      "     |  eos_token : hashable object or None, default None\n",
      "     |      The representation for end-of-sentence token.\n",
      "     |  mask_token : hashable object or None, default '[MASK]'\n",
      "     |      The representation for the special token of mask token for BERT.\n",
      "     |  sep_token : hashable object or None, default '[SEP]'\n",
      "     |      a token used to separate sentence pairs for BERT.\n",
      "     |  cls_token : hashable object or None, default '[CLS]'\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      BERTVocab\n",
      "     |      gluonnlp.vocab.vocab.Vocab\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, counter=None, max_size=None, min_freq=1, unknown_token='[UNK]', padding_token='[PAD]', bos_token=None, eos_token=None, mask_token='[MASK]', sep_token='[SEP]', cls_token='[CLS]', reserved_tokens=None, token_to_idx=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  from_json(json_str) from builtins.type\n",
      "     |      Deserialize BERTVocab object from json string.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      json_str : str\n",
      "     |          Serialized json string of a BERTVocab object.\n",
      "     |      \n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      BERTVocab\n",
      "     |  \n",
      "     |  from_sentencepiece(path, mask_token='[MASK]', sep_token='[SEP]', cls_token='[CLS]', unknown_token=None, padding_token=None, bos_token=None, eos_token=None, reserved_tokens=None) from builtins.type\n",
      "     |      BERTVocab from pre-trained sentencepiece Tokenizer\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      path : str\n",
      "     |          Path to the pre-trained subword tokenization model.\n",
      "     |      mask_token : hashable object or None, default '[MASK]'\n",
      "     |          The representation for the special token of mask token for BERT\n",
      "     |      sep_token : hashable object or None, default '[SEP]'\n",
      "     |          a token used to separate sentence pairs for BERT.\n",
      "     |      cls_token : hashable object or None, default '[CLS]'\n",
      "     |      unknown_token : hashable object or None, default None\n",
      "     |          The representation for any unknown token. In other words,\n",
      "     |          any unknown token will be indexed as the same representation.\n",
      "     |          If set to None, it is set to the token corresponding to the unk_id()\n",
      "     |          in the loaded sentencepiece model.\n",
      "     |      padding_token : hashable object or None, default '[PAD]'\n",
      "     |          The representation for padding token.\n",
      "     |      bos_token : hashable object or None, default None\n",
      "     |          The representation for the begin of sentence token.\n",
      "     |          If set to None, it is set to the token corresponding to the bos_id()\n",
      "     |          in the loaded sentencepiece model.\n",
      "     |      eos_token : hashable object or None, default None\n",
      "     |          The representation for the end of sentence token.\n",
      "     |          If set to None, it is set to the token corresponding to the bos_id()\n",
      "     |          in the loaded sentencepiece model.\n",
      "     |      reserved_tokens : list of strs or None, optional\n",
      "     |          A list of reserved tokens that will always be indexed.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      BERTVocab\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gluonnlp.vocab.vocab.Vocab:\n",
      "     |  \n",
      "     |  __call__(self, tokens)\n",
      "     |      Looks up indices of text tokens according to the vocabulary.\n",
      "     |      \n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      tokens : str or list of strs\n",
      "     |          A source token or tokens to be converted.\n",
      "     |      \n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      int or list of ints\n",
      "     |          A token index or a list of token indices according to the vocabulary.\n",
      "     |  \n",
      "     |  __contains__(self, token)\n",
      "     |      Checks whether a text token exists in the vocabulary.\n",
      "     |      \n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      token : str\n",
      "     |          A text token.\n",
      "     |      \n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      bool\n",
      "     |          Whether the text token exists in the vocabulary (including `unknown_token`).\n",
      "     |  \n",
      "     |  __getitem__(self, tokens)\n",
      "     |      Looks up indices of text tokens according to the vocabulary.\n",
      "     |      \n",
      "     |      If `unknown_token` of the vocabulary is None, looking up unknown tokens results in KeyError.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      tokens : str or list of strs\n",
      "     |          A source token or tokens to be converted.\n",
      "     |      \n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      int or list of ints\n",
      "     |          A token index or a list of token indices according to the vocabulary.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  set_embedding(self, *embeddings)\n",
      "     |      Attaches one or more embeddings to the indexed text tokens.\n",
      "     |      \n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      embeddings : None or tuple of :class:`gluonnlp.embedding.TokenEmbedding` instances\n",
      "     |          The embedding to be attached to the indexed tokens. If a tuple of multiple embeddings\n",
      "     |          are provided, their embedding vectors will be concatenated for the same token.\n",
      "     |  \n",
      "     |  to_indices(self, tokens)\n",
      "     |      Looks up indices of text tokens according to the vocabulary.\n",
      "     |      \n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      tokens : str or list of strs\n",
      "     |          A source token or tokens to be converted.\n",
      "     |      \n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      int or list of ints\n",
      "     |          A token index or a list of token indices according to the vocabulary.\n",
      "     |  \n",
      "     |  to_json(self)\n",
      "     |      Serialize Vocab object to json string.\n",
      "     |      \n",
      "     |      This method does not serialize the underlying embedding.\n",
      "     |  \n",
      "     |  to_tokens(self, indices)\n",
      "     |      Converts token indices to tokens according to the vocabulary.\n",
      "     |      \n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      indices : int or list of ints\n",
      "     |          A source token index or token indices to be converted.\n",
      "     |      \n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      str or list of strs\n",
      "     |          A token or a list of tokens according to the vocabulary.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from gluonnlp.vocab.vocab.Vocab:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  embedding\n",
      "     |  \n",
      "     |  idx_to_token\n",
      "     |  \n",
      "     |  reserved_tokens\n",
      "     |  \n",
      "     |  token_to_idx\n",
      "     |  \n",
      "     |  unknown_token\n",
      "    \n",
      "    class ByteSubwords(SubwordFunction)\n",
      "     |  ByteSubwords(encoding='utf-8')\n",
      "     |  \n",
      "     |  Map words to a list of bytes.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  encoding : str, default 'utf-8\n",
      "     |      Encoding to use for obtaining bytes.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ByteSubwords\n",
      "     |      SubwordFunction\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __call__(self, words)\n",
      "     |      Return a list of ndarrays of subwordindices for the given words.\n",
      "     |  \n",
      "     |  __init__(self, encoding='utf-8')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |      Return the number of subwords modeled.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  indices_to_subwords(self, subwordindices)\n",
      "     |      Return list of subwords associated with subword indices.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      subwordindices : iterable of int\n",
      "     |          Subword indices to look up.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      Iterable of str.\n",
      "     |  \n",
      "     |  subwords_to_indices(self, subwords)\n",
      "     |      Return list of subwordindices associated with subwords.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      subwords : iterable of str\n",
      "     |          Subwords to replace by indices.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      Iterable of int.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from SubwordFunction:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class ELMoCharVocab(builtins.object)\n",
      "     |  ELMoCharVocab(bos_token='<bos>', eos_token='<eos>')\n",
      "     |  \n",
      "     |  ELMo special character vocabulary\n",
      "     |  \n",
      "     |  The vocab aims to map individual tokens to sequences of character ids, compatible with ELMo.\n",
      "     |  To be consistent with previously trained models, we include it here.\n",
      "     |  \n",
      "     |  Specifically, char ids 0-255 come from utf-8 encoding bytes.\n",
      "     |  Above 256 are reserved for special tokens.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  bos_token : hashable object or None, default '<bos>'\n",
      "     |      The representation for the special token of beginning-of-sequence token.\n",
      "     |  eos_token : hashable object or None, default '<eos>'\n",
      "     |      The representation for the special token of end-of-sequence token.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  max_word_length : 50\n",
      "     |      The maximum number of character a word contains is 50 in ELMo.\n",
      "     |  bos_id : 256\n",
      "     |      The index of beginning of the sentence character is 256 in ELMo.\n",
      "     |  eos_id : 257\n",
      "     |      The index of end of the sentence character is 257 in ELMo.\n",
      "     |  bow_id : 258\n",
      "     |      The index of beginning of the word character is 258 in ELMo.\n",
      "     |  eow_id : 259\n",
      "     |      The index of end of the word character is 259 in ELMo.\n",
      "     |  pad_id : 260\n",
      "     |      The index of padding character is 260 in ELMo.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __call__(self, tokens)\n",
      "     |      Looks up indices of text tokens according to the vocabulary.\n",
      "     |      \n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      tokens : str or list of strs\n",
      "     |          A source token or tokens to be converted.\n",
      "     |      \n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      int or list of ints\n",
      "     |          A list of char indices or a list of list of char indices according to the vocabulary.\n",
      "     |  \n",
      "     |  __getitem__(self, tokens)\n",
      "     |      Looks up indices of text tokens according to the vocabulary.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      tokens : str or list of strs\n",
      "     |          A source token or tokens to be converted.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      int or list of ints\n",
      "     |          A list of char indices or a list of list of char indices according to the vocabulary.\n",
      "     |  \n",
      "     |  __init__(self, bos_token='<bos>', eos_token='<eos>')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  bos_id = 256\n",
      "     |  \n",
      "     |  bow_id = 258\n",
      "     |  \n",
      "     |  eos_id = 257\n",
      "     |  \n",
      "     |  eow_id = 259\n",
      "     |  \n",
      "     |  max_word_chars = 48\n",
      "     |  \n",
      "     |  max_word_length = 50\n",
      "     |  \n",
      "     |  pad_id = 260\n",
      "    \n",
      "    class NGramHashes(SubwordFunction)\n",
      "     |  NGramHashes(num_subwords, ngrams=(3, 4, 5, 6), special_tokens=None)\n",
      "     |  \n",
      "     |  Map words to a list of hashes in a restricted domain.\n",
      "     |  \n",
      "     |  The hash function is the same as in\n",
      "     |  https://github.com/facebookresearch/fastText\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  num_subwords : int\n",
      "     |      Size of target set for the hash function.\n",
      "     |  ngrams : list of int, default [3, 4, 5, 6]\n",
      "     |      n-s for which to hash the ngrams\n",
      "     |  special_tokens : set of str, default None\n",
      "     |      Set of words for which not to look up subwords.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      NGramHashes\n",
      "     |      SubwordFunction\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __call__(self, words)\n",
      "     |      Return a list of ndarrays of subwordindices for the given words.\n",
      "     |  \n",
      "     |  __init__(self, num_subwords, ngrams=(3, 4, 5, 6), special_tokens=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |      Return the number of subwords modeled.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  indices_to_subwords(self, subwordindices)\n",
      "     |      This raises RuntimeError because the subword function is not invertible.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      subwordindices : iterable of int\n",
      "     |          Subword indices to look up.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      Iterable of str.\n",
      "     |  \n",
      "     |  subwords_to_indices(self, subwords)\n",
      "     |      Return list of subwordindices associated with subwords.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      subwords : iterable of str\n",
      "     |          Subwords to replace by indices.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      Iterable of int.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  fasttext_hash_asbytes(ngram, encoding='utf-8')\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from SubwordFunction:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class SubwordFunction(builtins.object)\n",
      "     |  A SubwordFunction maps words to lists of subword indices.\n",
      "     |  \n",
      "     |  This class is abstract and to be subclassed. Use\n",
      "     |  gluonnlp.vocab.list_subword_functions to list all available subword\n",
      "     |  functions.\n",
      "     |  \n",
      "     |  A SubwordFunction object is callable and returns a list of ndarrays of\n",
      "     |  subwordindices for the given words in a call.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __call__(self, words)\n",
      "     |      Return a list of ndarrays of subwordindices for the given words.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |      Return the number of subwords modeled.\n",
      "     |  \n",
      "     |  indices_to_subwords(self, subwordindices)\n",
      "     |      Return list of subwords associated with subword indices.\n",
      "     |      \n",
      "     |      This may raise RuntimeError if the subword function is not invertible.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      subwordindices : iterable of int\n",
      "     |          Subword indices to look up.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      Iterable of str.\n",
      "     |  \n",
      "     |  subwords_to_indices(self, subwords)\n",
      "     |      Return list of subwordindices associated with subwords.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      subwords : iterable of str\n",
      "     |          Subwords to replace by indices.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      Iterable of int.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Vocab(builtins.object)\n",
      "     |  Vocab(counter: Union[gluonnlp.data.utils.Counter, NoneType] = None, max_size: Union[int, NoneType] = None, min_freq: int = 1, unknown_token: Union[Hashable, NoneType] = '<unk>', deprecated_padding_token: Union[Hashable, NoneType] = <object object at 0x00000251D6AD26E0>, deprecated_bos_token: Union[Hashable, NoneType] = <object object at 0x00000251D6AD26F0>, deprecated_eos_token: Union[Hashable, NoneType] = <object object at 0x00000251D6AD2700>, reserved_tokens: Union[List[Hashable], NoneType] = None, token_to_idx: Union[Dict[Hashable, int], NoneType] = None, *, padding_token: Union[Hashable, NoneType] = '<pad>', bos_token: Union[Hashable, NoneType] = '<bos>', eos_token: Union[Hashable, NoneType] = '<eos>', **kwargs)\n",
      "     |  \n",
      "     |  Indexing and embedding attachment for text tokens.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  counter\n",
      "     |      Counts text token frequencies in the text data. Its keys will be indexed according to\n",
      "     |      frequency thresholds such as `max_size` and `min_freq`. Keys of `counter`,\n",
      "     |      `unknown_token`, and values of `reserved_tokens` must be of the same hashable type.\n",
      "     |      Examples: str, int, and tuple.\n",
      "     |  max_size\n",
      "     |      The maximum possible number of the most frequent tokens in the keys of `counter` that can be\n",
      "     |      indexed. Note that this argument does not count any token from `reserved_tokens`. Suppose\n",
      "     |      that there are different keys of `counter` whose frequency are the same, if indexing all of\n",
      "     |      them will exceed this argument value, such keys will be indexed one by one according to\n",
      "     |      their __cmp__() order until the frequency threshold is met. If this argument is None or\n",
      "     |      larger than its largest possible value restricted by `counter` and `reserved_tokens`, this\n",
      "     |      argument has no effect.\n",
      "     |  min_freq\n",
      "     |      The minimum frequency required for a token in the keys of `counter` to be indexed.\n",
      "     |  unknown_token\n",
      "     |      The representation for any unknown token. If `unknown_token` is not\n",
      "     |      `None`, looking up any token that is not part of the vocabulary and\n",
      "     |      thus considered unknown will return the index of `unknown_token`. If\n",
      "     |      None, looking up an unknown token will result in `KeyError`.\n",
      "     |  reserved_tokens\n",
      "     |      A list specifying additional tokens to be added to the vocabulary.\n",
      "     |      `reserved_tokens` must not contain the value of `unknown_token` or\n",
      "     |      duplicate tokens. It must neither contain special tokens specified via\n",
      "     |      keyword arguments.\n",
      "     |  token_to_idx\n",
      "     |      If not `None`, specifies the indices of tokens to be used by the\n",
      "     |      vocabulary. Each token in `token_to_index` must be part of the Vocab\n",
      "     |      and each index can only be associated with a single token.\n",
      "     |      `token_to_idx` is not required to contain a mapping for all tokens. For\n",
      "     |      example, it is valid to only set the `unknown_token` index to 10\n",
      "     |      (instead of the default of 0) with `token_to_idx = {'<unk>': 10}`,\n",
      "     |      assuming that there are at least 10 tokens in the vocabulary.\n",
      "     |  `**kwargs`\n",
      "     |      Keyword arguments of the format `xxx_token` can be used to specify\n",
      "     |      further special tokens that will be exposed as attribute of the\n",
      "     |      vocabulary and associated with an index.\n",
      "     |      For example, specifying `mask_token='<mask>` as additional keyword\n",
      "     |      argument when constructing a vocabulary `v` leads to `v.mask_token`\n",
      "     |      exposing the value of the special token: `<mask>`.\n",
      "     |      If the specified token is not part of the Vocabulary, it will be added,\n",
      "     |      just as if it was listed in the `reserved_tokens` argument. The\n",
      "     |      specified tokens are listed together with reserved tokens in the\n",
      "     |      `reserved_tokens` attribute of the vocabulary object.\n",
      "     |  deprecated_padding_token\n",
      "     |      The representation for the special token of padding token. Default:\n",
      "     |      '<pad>'. Specifying padding_token as positional argument is deprecated\n",
      "     |      and support will be removed. Specify it as keyword argument instead\n",
      "     |      (see documentation of `**kwargs` above)\n",
      "     |  deprecated_bos_token\n",
      "     |      The representation for the special token of beginning-of-sequence\n",
      "     |      token. Default: '<bos>'. Specifying bos_token as positional argument is\n",
      "     |      deprecated and support will be removed. Specify it as keyword argument\n",
      "     |      instead (see documentation of `**kwargs` above)\n",
      "     |  deprecated_eos_token\n",
      "     |      The representation for the special token of end-of-sequence token.\n",
      "     |      Default: '<eos>'. Specifying eos_token as positional argument is\n",
      "     |      deprecated and support will be removed. Specify it as keyword argument\n",
      "     |      instead (see documentation of `**kwargs` above)\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  embedding : instance of :class:`gluonnlp.embedding.TokenEmbedding`\n",
      "     |      The embedding of the indexed tokens.\n",
      "     |  idx_to_token : list of strs\n",
      "     |      A list of indexed tokens where the list indices and the token indices are aligned.\n",
      "     |  reserved_tokens : list of strs or None\n",
      "     |      A list of reserved tokens that will always be indexed.\n",
      "     |  token_to_idx : dict mapping str to int\n",
      "     |      A dict mapping each token to its index integer.\n",
      "     |  unknown_token : hashable object or None\n",
      "     |      The representation for any unknown token. In other words, any unknown token will be indexed\n",
      "     |      as the same representation.\n",
      "     |  padding_token : hashable object or None\n",
      "     |      The representation for padding token.\n",
      "     |  bos_token : hashable object or None\n",
      "     |      The representation for beginning-of-sentence token.\n",
      "     |  eos_token : hashable object or None\n",
      "     |      The representation for end-of-sentence token.\n",
      "     |  \n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  \n",
      "     |  >>> text_data = ['hello', 'world', 'hello', 'nice', 'world', 'hi', 'world']\n",
      "     |  >>> counter = gluonnlp.data.count_tokens(text_data)\n",
      "     |  >>> my_vocab = gluonnlp.Vocab(counter)\n",
      "     |  >>> fasttext = gluonnlp.embedding.create('fasttext', source='wiki.simple')\n",
      "     |  -etc-\n",
      "     |  >>> my_vocab.set_embedding(fasttext)\n",
      "     |  >>> my_vocab.embedding[['hello', 'world']][:, :5]\n",
      "     |  <BLANKLINE>\n",
      "     |  [[ 0.39567   0.21454  -0.035389 -0.24299  -0.095645]\n",
      "     |   [ 0.10444  -0.10858   0.27212   0.13299  -0.33165 ]]\n",
      "     |  <NDArray 2x5 @cpu(0)>\n",
      "     |  >>> my_vocab[['hello', 'world']]\n",
      "     |  [5, 4]\n",
      "     |  \n",
      "     |  >>> input_dim, output_dim = my_vocab.embedding.idx_to_vec.shape\n",
      "     |  >>> layer = gluon.nn.Embedding(input_dim, output_dim)\n",
      "     |  >>> layer.initialize()\n",
      "     |  >>> layer.weight.set_data(my_vocab.embedding.idx_to_vec)\n",
      "     |  >>> layer(mx.nd.array([5, 4]))[:, :5]\n",
      "     |  <BLANKLINE>\n",
      "     |  [[ 0.39567   0.21454  -0.035389 -0.24299  -0.095645]\n",
      "     |   [ 0.10444  -0.10858   0.27212   0.13299  -0.33165 ]]\n",
      "     |  <NDArray 2x5 @cpu(0)>\n",
      "     |  >>> glove = gluonnlp.embedding.create('glove', source='glove.6B.50d')\n",
      "     |  -etc-\n",
      "     |  >>> my_vocab.set_embedding(glove)\n",
      "     |  >>> my_vocab.embedding[['hello', 'world']][:, :5]\n",
      "     |  <BLANKLINE>\n",
      "     |  [[-0.38497   0.80092   0.064106 -0.28355  -0.026759]\n",
      "     |   [-0.41486   0.71848  -0.3045    0.87445   0.22441 ]]\n",
      "     |  <NDArray 2x5 @cpu(0)>\n",
      "     |  \n",
      "     |  Extra keyword arguments of the format `xxx_token` are used to expose\n",
      "     |  specified tokens as attributes.\n",
      "     |  \n",
      "     |  >>> my_vocab2 = gluonnlp.Vocab(counter, special_token='hi')\n",
      "     |  >>> my_vocab2.special_token\n",
      "     |  'hi'\n",
      "     |  \n",
      "     |  With the `token_to_idx` argument the order of the `Vocab`'s index can be\n",
      "     |  adapted. For example, `Vocab` assigns the index `0` to the `unknown_token`\n",
      "     |  by default. With the `token_to_idx` argument, the default can be\n",
      "     |  overwritten. Here we assign index `3` to the unknown token representation\n",
      "     |  `<unk>`.\n",
      "     |  \n",
      "     |  >>> tok2idx = {'<unk>': 3}\n",
      "     |  >>> my_vocab3 = gluonnlp.Vocab(counter, token_to_idx=tok2idx)\n",
      "     |  >>> my_vocab3.unknown_token\n",
      "     |  '<unk>'\n",
      "     |  >>> my_vocab3[my_vocab3.unknown_token]\n",
      "     |  3\n",
      "     |  >>> my_vocab[my_vocab.unknown_token]\n",
      "     |  0\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __call__(self, tokens)\n",
      "     |      Looks up indices of text tokens according to the vocabulary.\n",
      "     |      \n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      tokens : str or list of strs\n",
      "     |          A source token or tokens to be converted.\n",
      "     |      \n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      int or list of ints\n",
      "     |          A token index or a list of token indices according to the vocabulary.\n",
      "     |  \n",
      "     |  __contains__(self, token)\n",
      "     |      Checks whether a text token exists in the vocabulary.\n",
      "     |      \n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      token : str\n",
      "     |          A text token.\n",
      "     |      \n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      bool\n",
      "     |          Whether the text token exists in the vocabulary (including `unknown_token`).\n",
      "     |  \n",
      "     |  __getitem__(self, tokens)\n",
      "     |      Looks up indices of text tokens according to the vocabulary.\n",
      "     |      \n",
      "     |      If `unknown_token` of the vocabulary is None, looking up unknown tokens results in KeyError.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      tokens : str or list of strs\n",
      "     |          A source token or tokens to be converted.\n",
      "     |      \n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      int or list of ints\n",
      "     |          A token index or a list of token indices according to the vocabulary.\n",
      "     |  \n",
      "     |  __init__(self, counter: Union[gluonnlp.data.utils.Counter, NoneType] = None, max_size: Union[int, NoneType] = None, min_freq: int = 1, unknown_token: Union[Hashable, NoneType] = '<unk>', deprecated_padding_token: Union[Hashable, NoneType] = <object object at 0x00000251D6AD26E0>, deprecated_bos_token: Union[Hashable, NoneType] = <object object at 0x00000251D6AD26F0>, deprecated_eos_token: Union[Hashable, NoneType] = <object object at 0x00000251D6AD2700>, reserved_tokens: Union[List[Hashable], NoneType] = None, token_to_idx: Union[Dict[Hashable, int], NoneType] = None, *, padding_token: Union[Hashable, NoneType] = '<pad>', bos_token: Union[Hashable, NoneType] = '<bos>', eos_token: Union[Hashable, NoneType] = '<eos>', **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  set_embedding(self, *embeddings)\n",
      "     |      Attaches one or more embeddings to the indexed text tokens.\n",
      "     |      \n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      embeddings : None or tuple of :class:`gluonnlp.embedding.TokenEmbedding` instances\n",
      "     |          The embedding to be attached to the indexed tokens. If a tuple of multiple embeddings\n",
      "     |          are provided, their embedding vectors will be concatenated for the same token.\n",
      "     |  \n",
      "     |  to_indices(self, tokens)\n",
      "     |      Looks up indices of text tokens according to the vocabulary.\n",
      "     |      \n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      tokens : str or list of strs\n",
      "     |          A source token or tokens to be converted.\n",
      "     |      \n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      int or list of ints\n",
      "     |          A token index or a list of token indices according to the vocabulary.\n",
      "     |  \n",
      "     |  to_json(self)\n",
      "     |      Serialize Vocab object to json string.\n",
      "     |      \n",
      "     |      This method does not serialize the underlying embedding.\n",
      "     |  \n",
      "     |  to_tokens(self, indices)\n",
      "     |      Converts token indices to tokens according to the vocabulary.\n",
      "     |      \n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      indices : int or list of ints\n",
      "     |          A source token index or token indices to be converted.\n",
      "     |      \n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      str or list of strs\n",
      "     |          A token or a list of tokens according to the vocabulary.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  from_json(json_str) from builtins.type\n",
      "     |      Deserialize Vocab object from json string.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      json_str : str\n",
      "     |          Serialized json string of a Vocab object.\n",
      "     |      \n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      Vocab\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  embedding\n",
      "     |  \n",
      "     |  idx_to_token\n",
      "     |  \n",
      "     |  reserved_tokens\n",
      "     |  \n",
      "     |  token_to_idx\n",
      "     |  \n",
      "     |  unknown_token\n",
      "\n",
      "FUNCTIONS\n",
      "    create_subword_function(subword_function_name, **kwargs)\n",
      "        Creates an instance of a subword function.\n",
      "    \n",
      "    list_subword_functions()\n",
      "        Get valid subword function names.\n",
      "    \n",
      "    register_subword_function(subword_cls)\n",
      "        Registers a new subword function.\n",
      "\n",
      "DATA\n",
      "    __all__ = ['Vocab', 'SubwordFunction', 'ByteSubwords', 'NGramHashes', ...\n",
      "\n",
      "FILE\n",
      "    c:\\users\\answl\\anaconda3\\envs\\nlp\\lib\\site-packages\\gluonnlp\\vocab\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d63351d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gluonnlp.vocab.bert.BERTVocab"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab.BERTVocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ebd34ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vocab(size=5, unk=\"[UNK]\", reserved=\"['[CLS]', '[SEP]', '[MASK]', '[PAD]']\")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab.BERTVocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51114afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'gluonnlp.vocab.bert.BERTVocab'>\n"
     ]
    }
   ],
   "source": [
    "print(nlp.vocab.BERTVocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29f7ab0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vocab(size=50000, unk=\"<unk>\", reserved=\"['<pad>', '<s>', '</s>']\")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b57748cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab(size=50000, unk=\"<unk>\", reserved=\"['<pad>', '<s>', '</s>']\")\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b618655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sent(seed_word, model, max_step=100, greedy=False, top_k=0, top_p=0.):\n",
    "    sent = seed_word # 문장 생성 시작 단어\n",
    "    toked = tokenizer(sent) # 해당 단어 토크나이즈 \n",
    "    # SentencepieceTokenizer(\"토크나이저 경로\")\n",
    "    # ex. tokenizer(\"이때\") --> ['▁이때']\n",
    "    \n",
    "    for _ in range(max_step): # 100번 반복\n",
    "        input_ids = tf.constant([vocab[vocab.bos_token],] + vocab[toked])[None, :]\n",
    "        # vocab[vocab.bos_token] = vocab['<s>'] = 0\n",
    "        # ex. vocab[toked] = vocab[tokenizer(\"이때\")] = vocab[['▁이때']] = [4499]\n",
    "        # [0,] + [4499] --> [0, 4499]\n",
    "        # tf.constant([0, 4499]) --> <tf.Tensor: shape=(2,), dtype=int32, numpy=array([   0, 4499])>\n",
    "        # array([   0, 4499])[None, :] --> array([[   0, 4499]]) # 차원 확장\n",
    "        \n",
    "        # (2)\n",
    "        # vocab[['▁이때', '문에']]--> [4499, 2592]\n",
    "        # input_ids: [0, 4499, 2592]--> array([[0, 4499, 2592]])\n",
    "        \n",
    "        outputs = model(input_ids)[:,-1,:]\n",
    "        # GPT2Model([[0, 4499]])\n",
    "        # 마지막 토큰의 은닉상태행렬: (1,50000) 크기\n",
    "        \n",
    "        if greedy: # (1) 단순히 가장 확률이 높은 단어를 선택하기\n",
    "            gen = vocab.to_tokens(tf.argmax(outputs, axis=-1).numpy().tolist()[0])\n",
    "            # tf.argmax([[-1.33..., 2.33,..., ...]]) --> array([0, 0, 0, ...])\n",
    "            # tf.argmax([[-1.33..., 2.33...., ...]], axis=-1) --> <tf.Tensor: ... array([2592])...>\n",
    "            # tf.argmax(__).numpy() --> array([2592])\n",
    "            # tf.argmax(__).numpy().tolist() --> [2592]\n",
    "            # tf.argmax(__).numpy().tolist()[0] --> 2592\n",
    "            # vocab.to_tokens(2592) --> '문에'\n",
    "            \n",
    "        else:      # (2) 확률이 높은 단어들 중 확률 분포에 따라 선택하기 \n",
    "            output_logit = tf_top_k_top_p_filtering(outputs[0], top_k=top_k, top_p=top_p)\n",
    "            gen = vocab.to_tokens(tf.random.categorical(output_logit, 1).numpy().tolist()[0])[0]\n",
    "            # tf.random.categorical([[2592]], 1)[0] --> [2592]\n",
    "            # vocab.to_tokens([2592]) --> ['문에'] --> [0] --> '문에'\n",
    "            # gen = '문에'\n",
    "        \n",
    "        # 문장 마지막('</s>')을 예측한다면 예측 반복 종료! \n",
    "        if gen == '</s>':\n",
    "            break\n",
    "        \n",
    "        sent += gen.replace('▁', ' ')\n",
    "        # 밑줄 표시를 띄어쓰기로 변경 후\n",
    "        # 원래 sent (생성 시작 단어)에 연결\n",
    "        toked = tokenizer(sent)\n",
    "        # 그리고 toked 새롭게 정의 \n",
    "        # ex. tokenizer(\"이때문에 \")\n",
    "        # --> ['▁이때', '문에']\n",
    "    \n",
    "    return sent\n",
    "\n",
    "def tf_top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-99999):\n",
    "    _logits = logits.numpy() # array([1.33, 2.22, ....])\n",
    "                             # TensorShape([50000])\n",
    "    top_k = min(top_k, logits.shape[-1])  # min(top_k, 50000)\n",
    "    \n",
    "    if top_k > 0: # 상위 k개 랭크를 고른다고 한다면 \n",
    "        indices_to_remove = logits < tf.math.top_k(logits, top_k)[0][..., -1, None]\n",
    "        # tf.math.top_k(outputs[0], 3)--> TopKV2(values=<tf.Tensor:...> , indices=<tf.Tensor:...>)\n",
    "        # ___[0]: <tf.Tensor: ... numpy=array([8.84.., 8.14.., 7.45...])...>\n",
    "        # ___[..., -1, None]: <...[7.45...]...>\n",
    "        # indices_to_remove : <... [True, True, True, ...] ...>\n",
    "        _logits[indices_to_remove] = filter_value\n",
    "        # array([-1.13.., 2.88.., ...]) --> 필터될 애들들\n",
    "        # 얘네들에 필터값 -99999 대입 --> _logits 베열 수정됨 \n",
    "        # _logits: array([-99999., -99999., -99999., .....])\n",
    "\n",
    "    if top_p > 0.0: # 특정 확률 이상의 것들로만 필터링한다고 한다면\n",
    "        # 1) 값 내림차순 정렬: 축적확률 보기 위해 \n",
    "        sorted_logits = tf.sort(logits, direction='DESCENDING')\n",
    "        # tf.Tensor 타입 내림차순 정렬!! \n",
    "        # ex. ...array([  8.840773 ,   8.142104 ,   7.456431 , ... ])...\n",
    "        sorted_indices = tf.argsort(logits, direction='DESCENDING')\n",
    "        # ex. ...array([ 2592, 47441, 47453, ...)...\n",
    "        cumulative_probs = tf.math.cumsum(tf.nn.softmax(sorted_logits, axis=-1), axis=-1)\n",
    "        # tf.nn.softmax(sorted_logits, axis=-1): array([6.9024004e-02, 3.4321975e-02, 1.7289728e-02, ...]) --> 소프트맥스 확률 변환\n",
    "        # tf.math.cumsum(__, axis=-1) : 축적 확률 - array([0.069024  , 0.10334598, 0.1206357 , ...\n",
    "        \n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        # 특정 확률(ex. 0.20) 넘어간다면(축적확률이 더 크다면) True 처리 --> 필터링할(버릴) 값\n",
    "        sorted_indices_to_remove = tf.concat([[False], sorted_indices_to_remove[..., :-1]], axis=0)\n",
    "        # __[..., :-1]: 마지막 필터링 여부값(True) 버림\n",
    "        # tf.concat([[False], [True, True, ...]], axis=0) --> [False, True, True, ...]\n",
    "        # 첫 값은 꼭 필터링 False --> 남김 !! \n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove].numpy().tolist()\n",
    "        # 제거할 인덱스들: array([18656,   512, 47462, ...,    73, 42568,     3])\n",
    "        _logits[indices_to_remove] = filter_value\n",
    "        # 해당 인덱스들에 필터값 -99999 대입 \n",
    "        # array([생존1, 생존2, ... , -99999, -99999])\n",
    "    \n",
    "    return tf.constant([_logits])\n",
    "    # array([[-99999., -99999., ..., 8.84077262878418, ... 8.142104148864746, ..., 7.456430912017822, ... -99999]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "1fc3bc38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4499, 2592]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[['▁이때', '문에']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5f5b62fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁이때', '문에']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('이때문에')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "7f703e0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(49990,), dtype=int32, numpy=array([18656,   512, 47462, ...,    73, 42568,     3])>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.argsort(outputs[0], direction='DESCENDING')[tf.concat([[False], sorted_indices_to_remove[..., :-1]], axis=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b17221d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=bool, numpy=array([False,  True,  True])>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.concat([[False], [True, True]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "fd958dd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(50000,), dtype=bool, numpy=array([False, False, False, ...,  True,  True,  True])>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_indices_to_remove = tf.math.cumsum(tf.nn.softmax(tf.sort(outputs[0], direction='DESCENDING'), axis=-1), axis=-1)>0.20\n",
    "sorted_indices_to_remove = tf.concat([[False], sorted_indices_to_remove[...,:-1]], axis=0)\n",
    "sorted_indices_to_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "06572756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(49999,), dtype=bool, numpy=array([False, False, False, ...,  True,  True,  True])>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[..., :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "bebe4826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(50000,), dtype=bool, numpy=array([False, False, False, ...,  True,  True,  True])>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.math.cumsum(tf.nn.softmax(tf.sort(outputs[0], direction='DESCENDING'), axis=-1), axis=-1)>0.20\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b839befa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(50000,), dtype=float32, numpy=\n",
       "array([0.069024  , 0.10334598, 0.1206357 , ..., 0.9999627 , 0.9999627 ,\n",
       "       0.9999627 ], dtype=float32)>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.math.cumsum(tf.nn.softmax(tf.sort(outputs[0], direction='DESCENDING'), axis=-1), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "44a9f004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(50000,), dtype=float32, numpy=\n",
       "array([6.9024004e-02, 3.4321975e-02, 1.7289728e-02, ..., 2.3193357e-11,\n",
       "       2.0838747e-11, 3.9051175e-12], dtype=float32)>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.nn.softmax(tf.sort(outputs[0], direction='DESCENDING'), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6c828d79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(50000,), dtype=int32, numpy=array([ 2592, 47441, 47453, ...,    73, 42568,     3])>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.argsort(outputs[0], direction='DESCENDING')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b514315d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(50000,), dtype=float32, numpy=\n",
       "array([  8.840773 ,   8.142104 ,   7.456431 , ..., -12.973081 ,\n",
       "       -13.0801325, -14.754661 ], dtype=float32)>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.sort(outputs[0], direction='DESCENDING')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "11a4db16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[47453]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.categorical(tf.constant([a]), 1).numpy().tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "68691e27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['문에']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.to_tokens(tf.random.categorical(tf.constant([a]), 1).numpy().tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c6b1fdbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=int64, numpy=array([[2592]], dtype=int64)>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.categorical(tf.constant([a]), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5e769936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8.84077262878418, 8.142104148864746, 7.456430912017822]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in a.tolist() if i != -99999.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "209b5936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 50000), dtype=float32, numpy=\n",
       "array([[-99999., -99999., -99999., ..., -99999., -99999., -99999.]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant([a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a61bd604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-99999., -99999., -99999., ..., -99999., -99999., -99999.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = outputs[0].numpy()\n",
    "\n",
    "a[outputs[0] < tf.math.top_k(outputs[0], 3)[0][..., -1, None]] = -99999\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "755b4f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[0].numpy()[outputs[0] < tf.math.top_k(outputs[0], 3)[0][..., -1, None]] = -99999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "74f8275b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -1.136622,   2.885709, -12.96938 , ...,  -6.681997,  -7.144026,\n",
       "        -5.017031], dtype=float32)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].numpy()[outputs[0] < tf.math.top_k(outputs[0], 3)[0][..., -1, None]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3cb7c328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(50000,), dtype=bool, numpy=array([ True,  True,  True, ...,  True,  True,  True])>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0] < tf.math.top_k(outputs[0], 3)[0][..., -1, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8d4f7971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=7.456431>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.math.top_k(outputs[0], 3)[0][..., -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fe903d2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=float32, numpy=array([7.456431], dtype=float32)>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.math.top_k(outputs[0], 3)[0][..., -1, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "64274a08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float32, numpy=array([8.840773, 8.142104, 7.456431], dtype=float32)>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.math.top_k(outputs[0], 3)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "950a90be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=8.840773>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0][2592]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a3a2d803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=8.142104>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0][47441]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ba50796d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TopKV2(values=<tf.Tensor: shape=(3,), dtype=float32, numpy=array([8.840773, 8.142104, 7.456431], dtype=float32)>, indices=<tf.Tensor: shape=(3,), dtype=int32, numpy=array([ 2592, 47441, 47453])>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.math.top_k(outputs[0], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d6514bfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([50000])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "49708c46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(50000,), dtype=float32, numpy=\n",
       "array([ -1.136622,   2.885709, -12.96938 , ...,  -6.681997,  -7.144026,\n",
       "        -5.017031], dtype=float32)>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bd545c98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'문에'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.to_tokens(2592)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a478c1f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Vocab.to_tokens of Vocab(size=50000, unk=\"<unk>\", reserved=\"['<pad>', '<s>', '</s>']\")>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.to_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "28af12f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(50000,), dtype=int64, numpy=array([0, 0, 0, ..., 0, 0, 0], dtype=int64)>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.argmax(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f9e4cd67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=int64, numpy=array([2592], dtype=int64)>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.argmax(outputs, axis = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f0237d73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2592], dtype=int64)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.argmax(outputs, axis = -1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "95c91c1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2592]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.argmax(outputs, axis = -1).numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "411cf1ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2592"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.argmax(outputs, axis = -1).numpy().tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "25ea9069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 50000), dtype=float32, numpy=\n",
       "array([[ -1.136622,   2.885709, -12.96938 , ...,  -6.681997,  -7.144026,\n",
       "         -5.017031]], dtype=float32)>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_model(tf.constant([0, 4499])[None, :])[:,-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4c7132f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = gpt_model(tf.constant([0, 4499])[None, :])[:,-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "433f4445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2, 50000), dtype=float32, numpy=\n",
       "array([[[  2.5147848,   4.4742403, -10.7896185, ...,  -5.0172014,\n",
       "          -6.3835497,  -5.6714473],\n",
       "        [ -1.136622 ,   2.885709 , -12.96938  , ...,  -6.681997 ,\n",
       "          -7.144026 ,  -5.017031 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_model(tf.constant([0, 4499])[None, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "05b75ad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=int32, numpy=array([   0, 4499])>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant([0, 4499])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "47716adc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[   0, 4499]])>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant([0, 4499])[None, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "28e61856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4499]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[['▁이때']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bf522b93",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate list (not \"int\") to list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_26028\\2430264504.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m4499\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: can only concatenate list (not \"int\") to list"
     ]
    }
   ],
   "source": [
    "tf.constant([0,] + )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f8fc74f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4499"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab['▁이때']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b930504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[vocab.bos_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac7ec34e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab['<s>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6396874d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab['</s>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "56d7bcc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab['<unk>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fb0edac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab['<pad>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e8c1c5c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vocab(size=50000, unk=\"<unk>\", reserved=\"['<pad>', '<s>', '</s>']\")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "35665abc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gluonnlp.vocab.bert.BERTVocab"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c778053e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'slice'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_26028\\3325882741.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\gluonnlp\\vocab\\vocab.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    397\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 398\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_token_to_idx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    399\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    400\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_token_to_idx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\gluonnlp\\data\\utils.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, k)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'slice'"
     ]
    }
   ],
   "source": [
    "vocab[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e01d4c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.bos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d33abd00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab['이때']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a978f895",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a73fb2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁이때']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"이때\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98f910c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'이때문에 일부 전문가들은 “이번 사건은 ‘제2의 삼성’을 꿈꾸는 삼성의 내부 사정을 잘 보여주는 사례”라고 평가했다.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sent(\"이때\", gpt_model, greedy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bce20dbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'이때 JPLH사는 윈도우8프로세서 칩을 채용한 그래픽카드를 출시한다.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sent(\"이때\", gpt_model, top_k=0, top_p=0.95) # Nucleus Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "8ccb8891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'이때 그녀가 몇 걸음 다가서자 곧바로 대답이 나온다.'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sent(\"이때\", gpt_model, top_k=0, top_p=0.95) # Nucleus Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "8b48180d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"이때 '메리트폴리오'는 LA타임스, CNBC등 대부분의 경제지들이 단기경제연구소(RED)라고 주목하지만 단기경제연구소는 웰스파고, JP모간 등 일부 유력지들은 정부와 외부지원을 받는 민간인 연구소는 부정적이라며 경계하고 있다.\""
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sent(\"이때\", gpt_model, top_k=0, top_p=0.95) # Nucleus Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "840d2629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'이때 문 후보는 \"안 후보가 단일화의 조건으로 정치쇄신을 내걸었는데, 안 후보가 이를 수용하지 않고 있다\"며 \"안 후보가 정치쇄신을 얘기하는데, 정치쇄신을 하려면 먼저 민주당이 먼저 쇄신하는 모습을 보여야 한다\"고 말했다.'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sent(\"이때\", gpt_model, top_k=0, top_p=0.20) # Nucleus Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "6b6975e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'이때도 주군은 자신이 어떤 인물인지, 어떤 일을 하는지, 어떤 상황인지, 어떤 인물인지 알지 못한다.'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sent(\"이때\", gpt_model, top_k=0, top_p=0.20) # Nucleus Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "80bfeedf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3), dtype=int64, numpy=array([[3, 4, 4]], dtype=int64)>"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.categorical(tf.constant([[1.0,2.0,3.0,4.0,5.0]]), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b487c1c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "13aa8a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['그때에 김첨지는 대수롭지 않은듯이,', '만일 김첨지가 주기를 띠지 않았던들 한 발을 대문에 들여놓았을 제 그곳을 지배하는 무시무시한 정적(靜寂) ― 폭풍우가 지나간 뒤의 바다 같은 정적이 다리가 떨렸으리라.']\n"
     ]
    }
   ],
   "source": [
    "DATA_IN_PATH = './data_in/KOR/'\n",
    "TRAIN_DATA_FILE = 'finetune_data.txt'\n",
    "\n",
    "sents = [s[:-1] for s in open(DATA_IN_PATH + TRAIN_DATA_FILE, encoding='utf8').readlines()]\n",
    "print(sents[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "08b3baf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = []\n",
    "output_data = []\n",
    "\n",
    "for s in sents:\n",
    "    tokens = [vocab[vocab.bos_token],] + vocab[tokenizer(s)] + [vocab[vocab.eos_token],]\n",
    "    # ex. ['<s>', '▁그때', '에', '▁김', '첨', '지는', \n",
    "    # '▁대수', '롭지', '▁않은', '듯이', ',', '</s>']\n",
    "    input_data.append(tokens[:-1])\n",
    "    output_data.append(tokens[1:])\n",
    "    \n",
    "# MAX_LEN : 30\n",
    "input_data = pad_sequences(input_data, MAX_LEN, value=vocab[vocab.padding_token]) # vocab['<pad>']\n",
    "output_data = pad_sequences(output_data, MAX_LEN, value=vocab[vocab.padding_token])\n",
    "\n",
    "input_data = np.array(input_data, dtype=np.int64)\n",
    "output_data = np.array(output_data, dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "f5520873",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\\\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, vocab[vocab.padding_token]))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "def accuracy_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, vocab[vocab.padding_token]))\n",
    "    mask = tf.expand_dims(tf.cast(mask, dtype=pred.dtype), axis=-1)\n",
    "    pred *= mask\n",
    "    acc = train_accuracy(real, pred)\n",
    "    \n",
    "    return tf.reduce_mean(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "98abce40",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_model.compile(loss=loss_function,\n",
    "                 optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "                 metrics=[accuracy_function])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "f475cf98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "16/16 [==============================] - 72s 3s/step - loss: 3.0223 - accuracy_function: 0.0993 - val_loss: 2.4907 - val_accuracy_function: 0.1127\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 52s 3s/step - loss: 2.5227 - accuracy_function: 0.1224 - val_loss: 2.3989 - val_accuracy_function: 0.1311\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 83s 5s/step - loss: 2.2738 - accuracy_function: 0.1391 - val_loss: 2.3849 - val_accuracy_function: 0.1453\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 84s 5s/step - loss: 2.0588 - accuracy_function: 0.1520 - val_loss: 2.3739 - val_accuracy_function: 0.1578\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 88s 6s/step - loss: 1.8566 - accuracy_function: 0.1654 - val_loss: 2.4125 - val_accuracy_function: 0.1699\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 85s 5s/step - loss: 1.6445 - accuracy_function: 0.1766 - val_loss: 2.4949 - val_accuracy_function: 0.1821\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - 87s 6s/step - loss: 1.4501 - accuracy_function: 0.1888 - val_loss: 2.5843 - val_accuracy_function: 0.1939\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - 82s 5s/step - loss: 1.2629 - accuracy_function: 0.2010 - val_loss: 2.6935 - val_accuracy_function: 0.2059\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - 83s 5s/step - loss: 1.0696 - accuracy_function: 0.2132 - val_loss: 2.7742 - val_accuracy_function: 0.2185\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - 86s 5s/step - loss: 0.8962 - accuracy_function: 0.2255 - val_loss: 2.9063 - val_accuracy_function: 0.2312\n"
     ]
    }
   ],
   "source": [
    "history = gpt_model.fit(input_data, output_data,\n",
    "                       batch_size=BATCH_SIZE, epochs=NUM_EPOCHS,\n",
    "                       validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "6e9b850c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁그때', '에', '▁김', '첨', '지는', '▁대수', '롭지', '▁않은', '듯이', ',']"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"그때에 김첨지는 대수롭지 않은듯이,\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68fc3c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "5b6a467a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "55943209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad>'"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.padding_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "00537ab1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab['<pad>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "c073b202",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the model checkpoint at ./data_out/KOR\\tf2_gpt2_finetuned_model.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "DATA_OUT_PATH = './data_out/KOR'\n",
    "model_name = \"tf2_gpt2_finetuned_model\"\n",
    "\n",
    "save_path = os.path.join(DATA_OUT_PATH, model_name)\n",
    "\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "    \n",
    "gpt_model.gpt2.save_pretrained(save_path)\n",
    "\n",
    "loaded_gpt_model = GPT2Model(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "1fbb0b28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'이때에 마침 길가 선술집에서 김첨지는 김첨지의 등쌀에 넘어갈 제 여유조차 없었다.'"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sent('이때', loaded_gpt_model, greedy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "b0172f11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'이때에 마침 길가 선술집에서 김첨지는 김첨지의 등쌀에 넘어갈 제 여유조차 없었다.'"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sent('이때', gpt_model, greedy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "23f1c7e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'이때에 오기가 좀 약한지 애걸복을 탄 듯 김첨지는 그 자리에 엎어져 눈을 크게 뜬 채 엉엉 울고 말았다.'"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sent('이때', loaded_gpt_model, top_k=0, top_p=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "26b6b9c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'이때문에 시장이 불안한 탓에 국고채 3년물 금리는 전날보다 0.04%포인트 오른 연 3.22%를 기록했다.'"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sent('이때', loaded_gpt_model, top_k=0, top_p=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "25a5ab43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'이때에 김첨지는 태자당을 뒤쫓아와 뜨개질 우는 가마니를 잡수시고 수염을 쓰다듬으며 너무 추워서 쓰러질 것 같았는데도 불구하고 아랑곳없이 화증을 내며 길바닥에 주저앉았다.'"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sent('이때', loaded_gpt_model, top_k=0, top_p=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "d292f022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'이때문에 박 대통령의 의중이 반영될 가능성이 없지 않아 있다.'"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sent('이때', loaded_gpt_model, top_k=0, top_p=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "ed2bd463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'이때문에 길가 선술집은 술렁이기 시작하였다.'"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sent('이때', loaded_gpt_model, top_k=0, top_p=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "9b7dc341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'이때에 이르러서는 그의 몸은 더욱 무거워졌고, 목은 더욱 무거워져 갔다.'"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sent('이때', loaded_gpt_model, top_k=0, top_p=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "e9686f8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'이때문에 동광학교(東光學校)에는 전교생이 3천 명 남짓하였다.'"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sent('이때', loaded_gpt_model, top_k=0, top_p=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "e19a48ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'이때에 이르러서는 마치 벌목꾼이 곡괭이를 들고 있는 양복쟁이를 끄는 것처럼 보였다.'"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sent('이때', loaded_gpt_model, top_k=0, top_p=0.20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
