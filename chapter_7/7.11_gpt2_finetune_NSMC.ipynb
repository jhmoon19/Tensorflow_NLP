{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c709e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf \n",
    "from transformers import TFGPT2Model\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import gluonnlp as nlp\n",
    "from gluonnlp.data import SentencepieceTokenizer\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f94f351b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED_NUM = 1234\n",
    "tf.random.set_seed(SEED_NUM)\n",
    "np.random.seed(SEED_NUM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a276df",
   "metadata": {},
   "source": [
    "## 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3abb421c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER_PATH = './gpt_ckpt/gpt2_kor_tokenizer.spiece'\n",
    "\n",
    "tokenizer = SentencepieceTokenizer(TOKENIZER_PATH)\n",
    "vocab = nlp.vocab.BERTVocab.from_sentencepiece(TOKENIZER_PATH,\n",
    "                                              mask_token=None,\n",
    "                                              sep_token='<unused0>',\n",
    "                                              cls_token=None,\n",
    "                                              unknown_token='<unk>',\n",
    "                                              padding_token='<pad>',\n",
    "                                              bos_token='<s>',\n",
    "                                              eos_token='</s>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5202636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total # dataset: train - 149995\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 3\n",
    "VALID_SPLIT = 0.1\n",
    "SENT_MAX_LEN = 39\n",
    "\n",
    "DATA_IN_PATH = './data_in/KOR'\n",
    "DATA_OUT_PATH = './data_out/KOR'\n",
    "\n",
    "DATA_TRAIN_PATH = os.path.join(DATA_IN_PATH, 'naver_movie', 'ratings_train.txt')\n",
    "DATA_TEST_PATH = os.path.join(DATA_IN_PATH, 'naver_movie', 'ratings_test.txt')\n",
    "\n",
    "\n",
    "train_data = pd.read_csv(DATA_TRAIN_PATH, header=0, sep='\\t', quoting=3)\n",
    "train_data = train_data.dropna()\n",
    "train_data.head()\n",
    "\n",
    "print(\"Total # dataset: train - {}\".format(len(train_data)))\n",
    "\n",
    "def clean_text(sent):\n",
    "    sent_clean = re.sub(\"[^가-힣ㄱ-ㅎㅏ-ㅣ\\\\s]\", \"\", sent)\n",
    "    return sent_clean\n",
    "\n",
    "train_data_sents = []\n",
    "train_data_labels = []\n",
    "\n",
    "for train_sent, train_label in train_data[['document', 'label']].values:\n",
    "    train_tokenized_text = vocab[tokenizer(clean_text(train_sent))]\n",
    "    \n",
    "    tokens = [vocab[vocab.bos_token]] # [0]\n",
    "    tokens += pad_sequences([train_tokenized_text],\n",
    "                           SENT_MAX_LEN, # 39\n",
    "                           value=vocab[vocab.padding_token], # 3\n",
    "                           padding='post').tolist()[0]\n",
    "    tokens += [vocab[vocab.eos_token]] # [0, ..., 1]\n",
    "    \n",
    "    train_data_sents.append(tokens)\n",
    "    train_data_labels.append(train_label)\n",
    "    \n",
    "train_data_sents = np.array(train_data_sents, dtype=np.int64)\n",
    "train_data_labels = np.array(train_data_labels, dtype=np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b96d86",
   "metadata": {},
   "source": [
    "## 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3886181f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFGPT2Classifier(tf.keras.Model):\n",
    "    def __init__(self, dir_path, num_class):\n",
    "        super(TFGPT2Classifier, self).__init__()\n",
    "        \n",
    "        self.gpt2 = TFGPT2Model.from_pretrained(dir_path)\n",
    "        self.num_class = num_class\n",
    "        \n",
    "        self.dropout = tf.keras.layers.Dropout(self.gpt2.config.summary_first_dropout)\n",
    "        self.classifier = tf.keras.layers.Dense(self.num_class,\n",
    "                                               kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=self.gpt2.config.initializer_range),\n",
    "                                               name=\"classifier\")\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        outputs = self.gpt2(inputs)\n",
    "        pooled_output = outputs[0][:, -1]\n",
    "        \n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d404b873",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing TFGPT2Model.\n",
      "\n",
      "All the weights of TFGPT2Model were initialized from the model checkpoint at ./gpt_ckpt.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFGPT2Model for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "BASE_MODEL_PATH = './gpt_ckpt'\n",
    "cls_model = TFGPT2Classifier(dir_path=BASE_MODEL_PATH, num_class=2)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=6.25e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "cls_model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d1acfb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data_out/KOR\\tf2_gpt2_naver_movie -- Folder already exists \n",
      "\n",
      "Epoch 1/3\n",
      "29/29 [==============================] - ETA: 0s - loss: 0.6976 - accuracy: 0.4900\n",
      "Epoch 1: val_accuracy improved from -inf to 0.42000, saving model to ./data_out/KOR\\tf2_gpt2_naver_movie\\weights.h5\n",
      "29/29 [==============================] - 208s 7s/step - loss: 0.6976 - accuracy: 0.4900 - val_loss: 0.7039 - val_accuracy: 0.4200\n",
      "Epoch 2/3\n",
      "29/29 [==============================] - ETA: 0s - loss: 0.6905 - accuracy: 0.5322 \n",
      "Epoch 2: val_accuracy improved from 0.42000 to 0.55000, saving model to ./data_out/KOR\\tf2_gpt2_naver_movie\\weights.h5\n",
      "29/29 [==============================] - 311s 11s/step - loss: 0.6905 - accuracy: 0.5322 - val_loss: 0.6926 - val_accuracy: 0.5500\n",
      "Epoch 3/3\n",
      "29/29 [==============================] - ETA: 0s - loss: 0.6874 - accuracy: 0.5811 \n",
      "Epoch 3: val_accuracy did not improve from 0.55000\n",
      "29/29 [==============================] - 321s 11s/step - loss: 0.6874 - accuracy: 0.5811 - val_loss: 0.6812 - val_accuracy: 0.5500\n"
     ]
    }
   ],
   "source": [
    "model_name = 'tf2_gpt2_naver_movie'\n",
    "\n",
    "earlystop_callback = EarlyStopping(monitor='val_accuracy', min_delta=0.0001, patience=2)\n",
    "\n",
    "checkpoint_path = os.path.join(DATA_OUT_PATH, model_name, 'weights.h5')\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    print(\"{} -- Folder already exists \\n\".format(checkpoint_dir))\n",
    "else:\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    print(\"{} -- Folder create complete \\n\".format(checkpoint_dir))\n",
    "    \n",
    "cp_callback = ModelCheckpoint(\n",
    "    checkpoint_path, monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "history = cls_model.fit(train_data_sents_short, \n",
    "                        train_data_labels_short, \n",
    "                        epochs=NUM_EPOCHS, \n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        validation_split=VALID_SPLIT, \n",
    "                        callbacks=[earlystop_callback, cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48ebbfc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149995"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e74414da",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_sents_short = train_data_sents[:1000]\n",
    "train_data_labels_short = train_data_labels[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ada7d40",
   "metadata": {},
   "source": [
    "## 모델 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "877eb670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num sents, labels 100, 100\n",
      "1/1 [==============================] - 5s 5s/step - loss: 0.6903 - accuracy: 0.7100\n",
      "test loss, test acc:  [0.6902951002120972, 0.7099999785423279]\n"
     ]
    }
   ],
   "source": [
    "test_data = pd.read_csv(DATA_TEST_PATH, header=0, delimiter='\\t', quoting=3)\n",
    "test_data = test_data.dropna() # 49997 테스트 데이터\n",
    "\n",
    "test_data = test_data[:100]\n",
    "\n",
    "test_data_sents = []\n",
    "test_data_labels = []\n",
    "\n",
    "for test_sent, test_label in test_data[['document','label']].values:\n",
    "    test_tokenized_text = vocab[tokenizer(clean_text(test_sent))]\n",
    "\n",
    "    tokens = [vocab[vocab.bos_token]]  \n",
    "    tokens += pad_sequences([test_tokenized_text], \n",
    "                            SENT_MAX_LEN, \n",
    "                            value=vocab[vocab.padding_token], \n",
    "                            padding='post').tolist()[0] \n",
    "    tokens += [vocab[vocab.eos_token]]\n",
    "\n",
    "    test_data_sents.append(tokens)\n",
    "    test_data_labels.append(test_label)\n",
    "\n",
    "test_data_sents = np.array(test_data_sents, dtype=np.int64)\n",
    "test_data_labels = np.array(test_data_labels, dtype=np.int64)\n",
    "\n",
    "print(\"num sents, labels {}, {}\".format(len(test_data_sents), len(test_data_labels)))\n",
    "\n",
    "cls_model.load_weights(checkpoint_path)\n",
    "\n",
    "results = cls_model.evaluate(test_data_sents, test_data_labels, batch_size=1024)\n",
    "print(\"test loss, test acc: \", results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
