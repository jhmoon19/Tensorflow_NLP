{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d91ef0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e779c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED_NUM = 1234\n",
    "tf.random.set_seed(SEED_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "619ab2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[MASK]', '[PAD]', '[SEP]', '[CLS]', '[UNK]'] \n",
      " [103, 0, 102, 101, 100]\n",
      "[101, 9521, 118741, 35506, 24982, 48549, 117, 9321, 118610, 119081, 48345, 119, 102]\n",
      "[101, 31178, 117, 11356, 106, 102]\n",
      "[CLS] 안녕하세요, 반갑습니다. [SEP]\n",
      "[CLS] Hello, world! [SEP]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\",\n",
    "                                         cache_dir='bert_ckpt',\n",
    "                                         do_lower_case=False)\n",
    "\n",
    "tf.random.set_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 3\n",
    "VALID_SPLIT = 0.2\n",
    "MAX_LEN = 28 * 2\n",
    "\n",
    "DATA_IN_PATH = 'data_in/KOR'\n",
    "DATA_OUT_PATH = 'data_out/KOR'\n",
    "\n",
    "print(tokenizer.all_special_tokens, '\\n', tokenizer.all_special_ids)\n",
    "# ['[PAD]', '[SEP]', '[MASK]', '[CLS]', '[UNK]']\n",
    "# [0, 102, 103, 101, 100]\n",
    "\n",
    "kor_encode = tokenizer.encode(\"안녕하세요, 반갑습니다.\")\n",
    "eng_encode = tokenizer.encode(\"Hello, world!\")\n",
    "\n",
    "kor_decode = tokenizer.decode(kor_encode)\n",
    "eng_decode = tokenizer.decode(eng_encode)\n",
    "\n",
    "print(kor_encode)\n",
    "print(eng_encode)\n",
    "print(kor_decode)\n",
    "print(eng_decode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14735cd1",
   "metadata": {},
   "source": [
    "# KorSTS Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6804baa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total # dataset: train - 5749, dev - 1500\n"
     ]
    }
   ],
   "source": [
    "TRAIN_STS_DF = os.path.join(DATA_IN_PATH, 'KorSTS', 'sts-train.tsv')\n",
    "DEV_STS_DF = os.path.join(DATA_IN_PATH, 'KorSTS', 'sts-dev.tsv')\n",
    "\n",
    "train_data = pd.read_csv(TRAIN_STS_DF, header=0, sep='\\t', quoting=3)\n",
    "dev_data = pd.read_csv(DEV_STS_DF, header=0, sep='\\t', quoting=3)\n",
    "\n",
    "print(\"Total # dataset: train - {}, dev - {}\".format(len(train_data), len(dev_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a51d7df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_tokenizer_v2(sent1, sent2, MAX_LEN):\n",
    "    \n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        text = sent1,\n",
    "        text_pair = sent2,\n",
    "        add_special_tokens = True,\n",
    "        max_length = MAX_LEN,\n",
    "        pad_to_max_length = True,\n",
    "        return_attention_mask = True,\n",
    "        truncation = True)\n",
    "    \n",
    "    input_id = encoded_dict['input_ids']\n",
    "    attention_mask = encoded_dict['attention_mask']\n",
    "    token_type_id = encoded_dict['token_type_ids']\n",
    "    \n",
    "    return input_id, attention_mask, token_type_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb0d7954",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(sent):\n",
    "    sent_clean = re.sub(\"[^a-zA-Z0-9ㄱ-ㅣ가-힣\\\\s]\", \" \", sent)\n",
    "    return sent_clean\n",
    "\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "token_type_ids = []\n",
    "data_labels = []\n",
    "\n",
    "for sent1, sent2, score in train_data[['sentence1', 'sentence2', 'score']].values:\n",
    "    try:\n",
    "        input_id, attention_mask, token_type_id = bert_tokenizer_v2(clean_text(sent1), clean_text(sent2), MAX_LEN)\n",
    "        input_ids.append(input_id)\n",
    "        attention_masks.append(attention_mask)\n",
    "        token_type_ids.append(token_type_id)\n",
    "        data_labels.append(score)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(sent1, sent2)\n",
    "        pass\n",
    "    \n",
    "train_input_ids = np.array(input_ids, dtype=int)\n",
    "train_attention_masks = np.array(attention_masks, dtype=int)\n",
    "train_type_ids = np.array(token_type_ids, dtype=int)\n",
    "train_inputs = (train_input_ids, train_attention_masks, train_type_ids)\n",
    "train_data_labels = np.array(data_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0db20150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['비행기가 이륙하고 있다.', '비행기가 이륙하고 있다.', 5.0],\n",
       "       ['한 남자가 큰 플루트를 연주하고 있다.', '남자가 플루트를 연주하고 있다.', 3.8],\n",
       "       ['한 남자가 피자에 치즈를 뿌려놓고 있다.', '한 남자가 구운 피자에 치즈 조각을 뿌려놓고 있다.', 3.8],\n",
       "       ...,\n",
       "       ['바레인으로 향하는 대통령', '시 주석 : 에볼라 퇴치를 계속 돕기 위한 중국', 0.0],\n",
       "       ['중국, 인도는 양국 관계를 증진시키겠다고 맹세한다',\n",
       "        '중국은 불안한 주식 거래자들을 안심시키기 위해 뒤뚱거리고 있다.', 0.0],\n",
       "       ['푸틴 대변인 : 도핑 혐의는 근거 없는 것으로 보인다.',\n",
       "        '가장 최근의 심한 날씨 : 토네이도 후 텍사스에서 1명 사망', 0.0]], dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[['sentence1', 'sentence2', 'score']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "70dd25c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5749, 3)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[['sentence1', 'sentence2', 'score']].values.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc164417",
   "metadata": {},
   "source": [
    "# DEV SET Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f65a9057",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = []\n",
    "attention_masks = []\n",
    "token_type_ids = []\n",
    "data_labels = []\n",
    "\n",
    "for sent1, sent2, score in dev_data[['sentence1', 'sentence2', 'score']].values:\n",
    "    try:\n",
    "        input_id, attention_mask, token_type_id = bert_tokenizer_v2(clean_text(sent1), clean_text(sent2), MAX_LEN)\n",
    "        input_ids.append(input_id)\n",
    "        attention_masks.append(attention_mask)\n",
    "        token_type_ids.append(token_type_id)\n",
    "        data_labels.append(score)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(sent1, sent2)\n",
    "        pass\n",
    "    \n",
    "dev_input_ids = np.array(input_ids, dtype=int)\n",
    "dev_attention_masks = np.array(attention_masks, dtype=int)\n",
    "dev_type_ids = np.array(token_type_ids, dtype=int)\n",
    "dev_inputs = (dev_input_ids, dev_attention_masks, dev_type_ids)\n",
    "dev_data_labels = np.array(data_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bf50aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# train labels: 5749, #dev labels: 1500\n"
     ]
    }
   ],
   "source": [
    "print(\"# train labels: {}, #dev labels: {}\".format(len(train_data_labels), len(dev_data_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2529738",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFBertRegressor(tf.keras.Model):\n",
    "    def __init__(self, model_name, dir_path, num_class):\n",
    "        super(TFBertRegressor, self).__init__()\n",
    "        \n",
    "        self.bert = TFBertModel.from_pretrained(model_name, cache_dir=dir_path)\n",
    "        self.num_class = num_class\n",
    "        self.dropout = tf.keras.layers.Dropout(self.bert.config.hidden_dropout_prob)\n",
    "        self.regressor = tf.keras.layers.Dense(self.num_class,\n",
    "                                              kernel_initializer=tf.keras.initializers.TruncatedNormal(self.bert.config.initializer_range),\n",
    "                                              name='regressor')\n",
    "        \n",
    "    def call(self, inputs, attention_mask=None, token_type_ids=None, training=False):\n",
    "        \n",
    "        outputs = self.bert(inputs, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.dropout(pooled_output, training=training)\n",
    "        logits = self.regressor(pooled_output)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19ba03ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the model checkpoint at bert-base-multilingual-cased.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "regression_model = TFBertRegressor(model_name='bert-base-multilingual-cased',\n",
    "                                  dir_path='bert_ckpt',\n",
    "                                  num_class=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6ccf9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PearsonCorrelationMetric(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name=\"pearson_correlation\", **kargs):\n",
    "        super(PearsonCorrelationMetric, self).__init__(name=name,**kargs)\n",
    "        self.y_true_list = []\n",
    "        self.y_pred_list = []\n",
    "        \n",
    "    \"\"\" 배치마다 나오는 결과값들을 평가함수에 적용 \"\"\"\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.reshape(y_true, shape=[-1])\n",
    "        y_pred = tf.reshape(y_pred, shape=[-1])\n",
    "        self.y_true_list.append(y_true)\n",
    "        self.y_pred_list.append(y_pred)\n",
    "    \n",
    "    \"\"\" 각 분포들을 하나의 벡터로 만들고 피어슨 상관계수 구함 \"\"\"\n",
    "    def result(self):\n",
    "        y_true = tf.concat(self.y_true_list, -1)\n",
    "        y_pred = tf.concat(self.y_pred_list, -1)\n",
    "        pearson_correlation = self.pearson(y_true, y_pred)\n",
    "        \n",
    "        return pearson_correlation\n",
    "    \n",
    "    \"\"\" 각 에폭 끝난 후 리스트 초기화 \"\"\"\n",
    "    def reset_states(self):\n",
    "        self.y_true_list = []\n",
    "        self.y_pred_list = []\n",
    "        \n",
    "    def pearson(self, true, pred):\n",
    "        m_true = tf.reduce_mean(true)\n",
    "        m_pred = tf.reduce_mean(pred)\n",
    "        m_true, m_pred = true - m_true, pred - m_pred\n",
    "        num = tf.reduce_sum(tf.multiply(m_true, m_pred))\n",
    "        den = tf.sqrt(tf.multiply(tf.reduce_sum(tf.square(m_true)),\n",
    "                                 tf.reduce_sum(tf.square(m_pred)))) + 1e-12\n",
    "        \n",
    "        return num / den"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24d6aa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(3e-5)\n",
    "loss = tf.keras.losses.MeanSquaredError()\n",
    "metric = PearsonCorrelationMetric()\n",
    "regression_model.compile(optimizer=optimizer, loss=loss, metrics=[metric], run_eagerly=True)\n",
    "# run eagerly: 사용자 평가 지표 모듈 사용 시 에러 발생하지 않도록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d49a0c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_out/KOR\\tf2_BERT_KorSTS -- Folder already exists \n",
      "\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\answl\\anaconda3\\envs\\nlp\\lib\\site-packages\\keras\\engine\\training.py:2086: UserWarning: Metric PearsonCorrelationMetric implements a `reset_states()` method; rename it to `reset_state()` (without the final \"s\"). The name `reset_states()` has been deprecated to improve API consistency.\n",
      "  m.reset_state()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - ETA: 0s - loss: 7.0894 - pearson_correlation: -0.0401 \n",
      "Epoch 1: val_pearson_correlation improved from -inf to -0.03929, saving model to data_out/KOR\\tf2_BERT_KorSTS\\weights.h5\n",
      "4/4 [==============================] - 168s 48s/step - loss: 7.0894 - pearson_correlation: -0.0401 - val_loss: 3.8617 - val_pearson_correlation: -0.0393\n",
      "Epoch 2/3\n",
      "4/4 [==============================] - ETA: 0s - loss: 2.2194 - pearson_correlation: 0.0343 \n",
      "Epoch 2: val_pearson_correlation improved from -0.03929 to 0.62759, saving model to data_out/KOR\\tf2_BERT_KorSTS\\weights.h5\n",
      "4/4 [==============================] - 206s 40s/step - loss: 2.2194 - pearson_correlation: 0.0343 - val_loss: 1.5128 - val_pearson_correlation: 0.6276\n",
      "Epoch 3/3\n",
      "4/4 [==============================] - ETA: 0s - loss: 1.8015 - pearson_correlation: 0.0832 \n",
      "Epoch 3: val_pearson_correlation improved from 0.62759 to 0.67898, saving model to data_out/KOR\\tf2_BERT_KorSTS\\weights.h5\n",
      "4/4 [==============================] - 224s 50s/step - loss: 1.8015 - pearson_correlation: 0.0832 - val_loss: 1.5706 - val_pearson_correlation: 0.6790\n",
      "{'loss': [7.0894246101379395, 2.219385862350464, 1.801527976989746], 'pearson_correlation': [-0.04010386019945145, 0.03434368222951889, 0.08315519988536835], 'val_loss': [3.8617241382598877, 1.5127766132354736, 1.5706486701965332], 'val_pearson_correlation': [-0.039288006722927094, 0.6275914311408997, 0.6789785027503967]}\n"
     ]
    }
   ],
   "source": [
    "#학습 진행하기\n",
    "model_name = \"tf2_BERT_KorSTS\"\n",
    "\n",
    "# overfitting을 막기 위한 ealrystop 추가\n",
    "earlystop_callback = EarlyStopping(monitor='val_pearson_correlation', min_delta=0.0001,patience=2,mode='max')\n",
    "# min_delta: the threshold that triggers the termination (acc should at least improve 0.0001)\n",
    "# patience: no improvment epochs (patience = 1, 1번 이상 상승이 없으면 종료)\\\n",
    "\n",
    "checkpoint_path = os.path.join(DATA_OUT_PATH, model_name, 'weights.h5')\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create path if exists\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    print(\"{} -- Folder already exists \\n\".format(checkpoint_dir))\n",
    "else:\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    print(\"{} -- Folder create complete \\n\".format(checkpoint_dir))\n",
    "    \n",
    "cp_callback = ModelCheckpoint(\n",
    "    checkpoint_path, monitor='val_pearson_correlation', verbose=1, save_best_only=True, save_weights_only=True,mode='max')\n",
    "\n",
    "# 학습과 eval 시작\n",
    "history = regression_model.fit(train_inputs_short, train_data_labels_short, epochs=NUM_EPOCHS,\n",
    "            validation_data = (dev_inputs_short, dev_data_labels_short),\n",
    "            batch_size=BATCH_SIZE, callbacks=[earlystop_callback, cp_callback])\n",
    "\n",
    "#steps_for_epoch\n",
    "print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb0a902c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs_short = tuple(i[:100] for i in train_inputs)\n",
    "train_data_labels_short = train_data_labels[:100]\n",
    "\n",
    "dev_inputs_short = tuple(i[:10] for i in dev_inputs)\n",
    "dev_data_labels_short = dev_data_labels[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b656c0d3",
   "metadata": {},
   "source": [
    "그냥 하면 180배치, 2시간 반 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0898d08d",
   "metadata": {},
   "source": [
    "# KorSTS Test Datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "780f85a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           genre filename      year  id  score                sentence1  \\\n",
      "0  main-captions   MSRvid  2012test  24    2.5     한 소녀가 머리를 스타일링하고 있다.   \n",
      "1  main-captions   MSRvid  2012test  33    3.6  한 무리의 남자들이 해변에서 축구를 한다.   \n",
      "2  main-captions   MSRvid  2012test  45    5.0  한 여성이 다른 여성의 발목을 재고 있다.   \n",
      "3  main-captions   MSRvid  2012test  63    4.2        한 남자가 오이를 자르고 있다.   \n",
      "4  main-captions   MSRvid  2012test  66    1.5       한 남자가 하프를 연주하고 있다.   \n",
      "\n",
      "                    sentence2  \n",
      "0            한 소녀가 머리를 빗고 있다.  \n",
      "1  한 무리의 소년들이 해변에서 축구를 하고 있다.  \n",
      "2      한 여자는 다른 여자의 발목을 측정한다.  \n",
      "3           한 남자가 오이를 자르고 있다.  \n",
      "4         한 남자가 키보드를 연주하고 있다.  \n",
      "# sents: 1379, # labels: 1379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\answl\\anaconda3\\envs\\nlp\\lib\\site-packages\\keras\\engine\\training.py:2086: UserWarning: Metric PearsonCorrelationMetric implements a `reset_states()` method; rename it to `reset_state()` (without the final \"s\"). The name `reset_states()` has been deprecated to improve API consistency.\n",
      "  m.reset_state()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 85s 25s/step - loss: 2.8333 - pearson_correlation: 0.3584\n",
      "test loss, test pearson correlation:  [2.8333382606506348, 0.3583824038505554]\n"
     ]
    }
   ],
   "source": [
    "TEST_STS_DF = os.path.join(DATA_IN_PATH, 'KorSTS', 'sts-test.tsv')\n",
    "\n",
    "test_data = pd.read_csv(TEST_STS_DF, header=0, delimiter = '\\t', quoting = 3)\n",
    "print(test_data.head())\n",
    "\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "token_type_ids = []\n",
    "data_labels = []\n",
    "\n",
    "for sent1, sent2, score in test_data[['sentence1', 'sentence2', 'score']].values:\n",
    "    try:\n",
    "        input_id, attention_mask, token_type_id = bert_tokenizer_v2(clean_text(sent1), clean_text(sent2), MAX_LEN)\n",
    "        input_ids.append(input_id)\n",
    "        attention_masks.append(attention_mask)\n",
    "        token_type_ids.append(token_type_id)\n",
    "        data_labels.append(score)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(sent1, sent2)\n",
    "        pass\n",
    "    \n",
    "test_input_ids = np.array(input_ids, dtype=int)\n",
    "test_attention_masks = np.array(attention_masks, dtype=int)\n",
    "test_type_ids = np.array(token_type_ids, dtype=int)\n",
    "test_inputs = (test_input_ids, test_attention_masks, test_type_ids)\n",
    "test_data_labels = np.array(data_labels)\n",
    "\n",
    "print(\"# sents: {}, # labels: {}\".format(len(test_input_ids), len(test_data_labels)))\n",
    "\n",
    "regression_model.load_weights(checkpoint_path)\n",
    "\n",
    "results = regression_model.evaluate(test_inputs, test_data_labels, batch_size=512)\n",
    "print(\"test loss, test pearson correlation: \", results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
